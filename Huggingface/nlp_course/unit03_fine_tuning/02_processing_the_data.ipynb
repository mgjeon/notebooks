{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25240e7-655d-4890-9528-d3af1c0deb68",
   "metadata": {},
   "source": [
    "# Processing the data\n",
    "\n",
    "- https://huggingface.co/learn/nlp-course/chapter3/2?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616f989a-0359-493a-9bbc-c4e27131efa1",
   "metadata": {},
   "source": [
    "Here is how we would train a sequence classifier on one batch in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84efdd8d-6f3d-4517-9a1d-ec872e7b2744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Same as before\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# This is new\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6079c75-6b58-48d2-be20-96ef738f1eb9",
   "metadata": {},
   "source": [
    "Of course, just training the model on two sentences is not going to yield very good results. To get better results, you will need to prepare a bigger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9512b82-fdd3-422f-9292-34748ee167e8",
   "metadata": {},
   "source": [
    "In this section we will use as an example the MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in a [paper](https://www.aclweb.org/anthology/I05-5002.pdf) by William B. Dolan and Chris Brockett. The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). We’ve selected it for this chapter because it’s a small dataset, so it’s easy to experiment with training on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c2473-2488-432c-bd9d-670d3a0dc2e0",
   "metadata": {},
   "source": [
    "## Loading a dataset from the Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a943de8-ba0b-4a79-bae6-1eca895d039a",
   "metadata": {},
   "source": [
    "General Language Understanding Evaluation (GLUE) benchmark: an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d08e76-0f66-4769-8769-cb002a1ab829",
   "metadata": {},
   "source": [
    "We can download the MRPC dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4fc72e-4b96-4673-8163-d1877dc8e35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db8fa63-170a-411f-8e3a-ad2f18654f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab874376-42ab-42f8-ad44-023bc78fa0ea",
   "metadata": {},
   "source": [
    "We can see the labels are already integers, so we won’t have to do any preprocessing there. To know which integer corresponds to which label, we can inspect the `features` of our `raw_train_dataset`. This will tell us the type of each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c7523a-bd7c-4c42-bf38-7c4cb6f09dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b887e0f-55f9-4f49-b10b-9be5179b1dfc",
   "metadata": {},
   "source": [
    "Behind the scenes, `label` is of type `ClassLabel`, and the mapping of integers to label name is stored in the `names` folder. `0` corresponds to `not_equivalent`, and `1` corresponds to `equivalent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0dd829-39e6-43ac-bfac-c72a1d39226e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Rudder was most recently senior vice president for the Developer & Platform Evangelism Business .',\n",
       " 'sentence2': 'Senior Vice President Eric Rudder , formerly head of the Developer and Platform Evangelism unit , will lead the new entity .',\n",
       " 'label': 0,\n",
       " 'idx': 16}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0fa5c8-f030-4dc7-ba61-611e1cac8068",
   "metadata": {},
   "source": [
    "## Preprocessing a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a9fb9-641e-4f22-ad71-c811853d606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e5244a-ea7b-47b6-8b97-4b9cd292d7d5",
   "metadata": {},
   "source": [
    "However, we can’t just pass two sequences to the model and get a prediction of whether the two sentences are paraphrases or not. We need to handle the two sequences as a pair, and apply the appropriate preprocessing. Fortunately, the tokenizer can also take a pair of sequences and prepare it the way our BERT model expects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef76d7-c86a-4609-8bbc-509382fecc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f65c43-9cea-4eb3-8ffb-8614640f5fdf",
   "metadata": {},
   "source": [
    "If we decode the IDs inside input_ids back to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f919819b-7ffe-49ce-81f3-f5e17176e9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11bd688-7bd5-47f3-98c6-ae90e648ffb4",
   "metadata": {},
   "source": [
    "So we see the model expects the inputs to be of the form `[CLS] sentence1 [SEP] sentence2 [SEP]` when there are two sentences. Aligning this with the `token_type_ids` gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4ab2f-d086-44fd-8aef-4085ddca49c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))\n",
    "print(inputs['token_type_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a87a854-631c-4473-835c-2ecbf97e90a6",
   "metadata": {},
   "source": [
    "As you can see, the parts of the input corresponding to `[CLS] sentence1 [SEP]` all have a token type ID of 0, while the other parts, corresponding to `sentence2 [SEP]`, all have a token type ID of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a133fb0d-1bb2-4b40-b268-741b10085398",
   "metadata": {},
   "source": [
    "We can feed the tokenizer a list of pairs of sentences by giving it the list of first sentences, then the list of second sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2a134-d85d-4d7a-abb1-72ab7cc3bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f3b1a9-0f3a-4cb5-8786-f1f3c0b88714",
   "metadata": {},
   "source": [
    "This works well, but it has the disadvantage of returning a dictionary (with our keys, input_ids, attention_mask, and token_type_ids, and values that are lists of lists). It will also only work if you have enough RAM to store your whole dataset during the tokenization (whereas the datasets from the 🤗 Datasets library are Apache Arrow files stored on the disk, so you only keep the samples you ask for loaded in memory).\n",
    "\n",
    "To keep the data as a dataset, we will use the Dataset.map() method. This also allows us some extra flexibility, if we need more preprocessing done than just tokenization. The map() method works by applying a function on each element of the dataset, so let’s define a function that tokenizes our inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a23e3-a2f9-4cb0-a9a3-dedea3d36364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2419e570-0c2c-4397-b05c-094fc552c7c6",
   "metadata": {},
   "source": [
    "Note that we’ve left the padding argument out in our tokenization function for now. This is because padding all the samples to the maximum length is not efficient: it’s better to pad the samples when we’re building a batch, as then we only need to pad to the maximum length in that batch, and not the maximum length in the entire dataset. This can save a lot of time and processing power when the inputs have very variable lengths!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec7fc0a-9af7-4a2a-a5e0-739327bb98bd",
   "metadata": {},
   "source": [
    "Here is how we apply the tokenization function on all our datasets at once. We’re using batched=True in our call to map so the function is applied to multiple elements of our dataset at once, and not on each element separately. This allows for faster preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b7662a-ad2b-4b52-869e-49f92c94cc37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8fb173-17e5-43b9-a20a-afa8c631a10b",
   "metadata": {},
   "source": [
    "The last thing we will need to do is pad all the examples to the length of the longest element when we batch elements together — a technique we refer to as dynamic padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc42ce4-69ac-45b7-a671-fcf789a9b4b0",
   "metadata": {},
   "source": [
    "## Dynamic padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a073f026-d7a0-4499-989c-48921c7570a7",
   "metadata": {},
   "source": [
    "The function that is responsible for putting together samples inside a batch is called a collate function. It’s an argument you can pass when you build a DataLoader, the default being a function that will just convert your samples to PyTorch tensors and concatenate them (recursively if your elements are lists, tuples, or dictionaries). This won’t be possible in our case since the inputs we have won’t all be of the same size. We have deliberately postponed the padding, to only apply it as necessary on each batch and avoid having over-long inputs with a lot of padding. This will speed up training by quite a bit, but note that if you’re training on a TPU it can cause problems — TPUs prefer fixed shapes, even when that requires extra padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f916f-da1a-49d9-8580-184a45656bb2",
   "metadata": {},
   "source": [
    "To do this in practice, we have to define a collate function that will apply the correct amount of padding to the items of the dataset we want to batch together. Fortunately, the 🤗 Transformers library provides us with such a function via DataCollatorWithPadding. It takes a tokenizer when you instantiate it (to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs) and will do everything you need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f547e-fb1c-4520-83ce-021ff75e1426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debfb49d-894b-4084-b3dd-9e5e701b7ce6",
   "metadata": {},
   "source": [
    "To test this new toy, let’s grab a few samples from our training set that we would like to batch together. Here, we remove the columns idx, sentence1, and sentence2 as they won’t be needed and contain strings (and we can’t create tensors with strings) and have a look at the lengths of each entry in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caa3a1e-c3ae-4305-b0c9-ca667ad342d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea9a26-8772-438b-bfd3-cc5a0fb4a8f5",
   "metadata": {},
   "source": [
    "No surprise, we get samples of varying length, from 32 to 67. Dynamic padding means the samples in this batch should all be padded to a length of 67, the maximum length inside the batch. Without dynamic padding, all of the samples would have to be padded to the maximum length in the whole dataset, or the maximum length the model can accept. Let’s double-check that our data_collator is dynamically padding the batch properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2709d8a8-c5f4-443c-bd5c-bd4e5305e3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8696503-8b72-4a18-b9e7-c1e638eaf26b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
