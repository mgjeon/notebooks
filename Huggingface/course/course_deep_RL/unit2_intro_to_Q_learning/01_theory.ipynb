{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18b37cc1-18f6-4ccd-a6c2-ab7b294b6deb",
   "metadata": {},
   "source": [
    "# Introduction to Q-Learning\n",
    "\n",
    "- https://huggingface.co/learn/deep-rl-course/unit2/introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d036019f-732d-4b41-bf41-93dee38fd480",
   "metadata": {},
   "source": [
    "In this unit, we’re going to dive deeper into one of the Reinforcement Learning methods: value-based methods and study our first RL algorithm: Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0555b475-720f-4936-a550-ae5f34502c4e",
   "metadata": {},
   "source": [
    "We’ll also implement our first RL agent from scratch, a Q-Learning agent, and will train it in two environments:\n",
    "\n",
    "- Frozen-Lake-v1 (non-slippery version): where our agent will need to go from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoiding holes (H).\n",
    "- An autonomous taxi: where our agent will need to learn to navigate a city to transport its passengers from point A to point B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbf4378-0684-45b6-9c5c-8904cd1d0dcc",
   "metadata": {},
   "source": [
    "# What is RL? A short recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d8a94f-d92c-4dd9-bc47-aadb71d42b9a",
   "metadata": {},
   "source": [
    "In RL, we build an agent that can make smart decisions.\n",
    "\n",
    "For instance, an agent that learns to play a video game. Or a trading agent that learns to maximize its benefits by deciding on what stocks to buy and when to sell.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/rl-process.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bace28ca-8416-438e-ac45-46593f9c4f84",
   "metadata": {},
   "source": [
    "To make intelligent decisions, our agent will learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback.\n",
    "\n",
    "Its goal is to maximize its expected cumulative reward (because of the reward hypothesis: a goal can be described as the maximization of the expected cumulative reward.).\n",
    "\n",
    "The agent’s decision-making process is called the policy π: given a state, a policy will output an action or a probability distribution over actions. That is, given an observation of the environment, a policy will provide an action (or multiple probabilities for each action) that the agent should take.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/policy.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e25083-c216-4c2b-bfe8-be4ea5d99a65",
   "metadata": {},
   "source": [
    "Our goal is to find an optimal policy π* , aka., a policy that leads to the best expected cumulative reward.\n",
    "\n",
    "And to find this optimal policy (hence solving the RL problem), there are two main types of RL methods:\n",
    "\n",
    "- Policy-based methods: Train the policy directly to learn which action to take given a state.\n",
    "- Value-based methods: Train a value function to learn which state is more valuable and use this value function to take the action that leads to it.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches.jpg\" width=500>\n",
    "\n",
    "And in this unit, we’ll dive deeper into the value-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e22b68-8b21-4b5a-986d-c83dd7eb74dc",
   "metadata": {},
   "source": [
    "# Two types of value-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f662b5-ba2e-43f0-afd7-098696a44b02",
   "metadata": {},
   "source": [
    "In value-based methods, we learn a value function that maps a state to the expected value of being at that state.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/vbm-1.jpg\" width=500>\n",
    "\n",
    "The value of a state is the expected discounted return the agent can get if it starts at that state and then acts according to our policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca3207-1849-449f-852a-ddc541fb44b8",
   "metadata": {},
   "source": [
    "Remember that the goal of an RL agent is to have an optimal policy π*.\n",
    "\n",
    "To find the optimal policy, we learned about two different methods:\n",
    "\n",
    "- Policy-based methods: Directly train the policy to select what action to take given a state (or a probability distribution over actions at that state). In this case, we don’t have a value function.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches-2.jpg\" width=500>\n",
    "\n",
    "The policy takes a state as input and outputs what action to take at that state (deterministic policy: a policy that output one action given a state, contrary to stochastic policy that output a probability distribution over actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd56dfe-e403-491c-85eb-3f46a9f38443",
   "metadata": {},
   "source": [
    "- Value-based methods: Indirectly, by training a value function that outputs the value of a state or a state-action pair. Given this value function, our policy will take an action.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches-3.jpg\" width=500>\n",
    "\n",
    "> Given a state, our action-value function (that we train) outputs the value of each action at that state. Then, our pre-defined Greedy Policy selects the action that will yield the highest value given a state or a state action pair.\n",
    "\n",
    "Since the policy is not trained/learned, we need to specify its behavior. For instance, if we want a policy that, given the value function, will take actions that always lead to the biggest reward, we’ll create a Greedy Policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7feeba-9f1d-49ad-a815-30f92e798a08",
   "metadata": {},
   "source": [
    "Consequently, **whatever method you use to solve your problem, you will have a policy**. In the case of value-based methods, you don’t train the policy: your policy is just a simple pre-specified function (for instance, the Greedy Policy) that uses the values given by the value-function to select its actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e7361-f224-47ea-b47f-e51117d89dde",
   "metadata": {},
   "source": [
    "So the difference is:\n",
    "\n",
    "- In policy-based training, the optimal policy (denoted π*) is found by training the policy directly.\n",
    "- In value-based training, finding an optimal value function (denoted Q* or V*, we’ll study the difference below) leads to having an optimal policy.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c737ae6-aeb6-4068-a4ae-1c158821c503",
   "metadata": {},
   "source": [
    "In fact, most of the time, in value-based methods, you’ll use an Epsilon-Greedy Policy that handles the exploration/exploitation trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f52e11-a40d-4f07-b72b-47b2edf06a35",
   "metadata": {},
   "source": [
    "As we mentioned above, we have two types of value-based functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2037fc-f6eb-42f4-8e1d-d50b7f5f7ab9",
   "metadata": {},
   "source": [
    "## The state-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9b87c1-ab53-4f33-8dba-0fe37179fa85",
   "metadata": {},
   "source": [
    "We write the state value function under a policy π like this:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/state-value-function-1.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c800d-872a-49b4-9981-519516a6d76c",
   "metadata": {},
   "source": [
    "For each state, the state-value function outputs the expected return if the agent starts at that state and then follows the policy forever afterward (for all future timesteps, if you prefer).\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/state-value-function-2.jpg\" width=500>\n",
    "\n",
    "> If we take the state with value -7: it's the expected return starting at that state and taking actions according to our policy (greedy policy), so right, right, right, down, down, right, right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d2c2b-4681-4242-9351-597c552da28a",
   "metadata": {},
   "source": [
    "## The action-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3998815c-134d-4f21-9183-ffc0170f45d4",
   "metadata": {},
   "source": [
    "In the action-value function, for each state and action pair, the action-value function outputs the expected return if the agent starts in that state, takes that action, and then follows the policy forever after.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/action-state-value-function-1.jpg\" width=500>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/action-state-value-function-2.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ac087-9645-4d5a-91f7-ac9a4f49bbff",
   "metadata": {},
   "source": [
    "We see that the difference is:\n",
    "- For the state-value function, we calculate the value of a state $S_t$\n",
    "- For the action-value function, we calculate the value of the state-action pair $(S_t, A_t)$ hence the value of taking that action at that state.\n",
    "\n",
    "In either case, whichever value function we choose (state-value or action-value function), the returned value is the expected return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32951897-0a75-445e-a506-9381d86899cb",
   "metadata": {},
   "source": [
    "However, the problem is that to calculate EACH value of a state or a state-action pair, we need to sum all the rewards an agent can get if it starts at that state.\n",
    "\n",
    "This can be a computationally expensive process, and that’s where the Bellman equation comes in to help us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89378b2c-130e-4dd4-a990-7048c563587d",
   "metadata": {},
   "source": [
    "# The Bellman Equation: simplify our value estimation\n",
    "\n",
    "The Bellman equation simplifies our state value or state-action value calculation.\n",
    "\n",
    "With what we have learned so far, we know that if we calculate $V(S_t)$ (the value of a state), we need to calculate the return starting at that state and then follow the policy forever after.\n",
    "\n",
    "The Bellman equation is a recursive equation that works like this: instead of starting for each state from the beginning and calculating the return, we can consider the value of any state as:\n",
    "- **The immediate reward $R_{t+1}$ + The discounted value of the state that follows $\\gamma * V(S_{t+1})$**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman4.jpg\" width=500>\n",
    "\n",
    "To recap, the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, which is a long process, we calculate the value as the sum of immediate reward + the discounted value of the state that follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6236a6-a0bf-42cf-973a-31d1ad1b3bcb",
   "metadata": {},
   "source": [
    "# Monte Carlo vs Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44a6ab-dbf5-4549-920c-29a6e81d225f",
   "metadata": {},
   "source": [
    "Remember that an RL agent learns by interacting with its environment. The idea is that given the experience and the received reward, the agent will update its value function or policy.\n",
    "\n",
    "Monte Carlo and Temporal Difference Learning are two different strategies on how to train our value function or our policy function. Both of them use experience to solve the RL problem.\n",
    "\n",
    "- Monte Carlo uses an entire episode of experience before learning.\n",
    "- Temporal Difference uses only a step $(S_t, A_t, R_{t+1}, S_{t+1})$ to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad9f044-7375-4de3-832a-2f267ff8ae54",
   "metadata": {},
   "source": [
    "## Monte Carlo: learning at the end of the episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3056e3e1-e2f6-443a-a69c-395202a58fad",
   "metadata": {},
   "source": [
    "Monte Carlo waits until the end of the episode, calculates $G_t$ (return) and uses it as a tagert for updating $V(S_t)$.\n",
    "\n",
    "So it requires a complete episode of interaction before updating our value function.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/monte-carlo-approach.jpg\" width=500>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-3p.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95147c9-f960-447b-acac-7eae97640112",
   "metadata": {},
   "source": [
    "## Temporal Difference Learning: learning at each step\n",
    "\n",
    "Temporal Difference, on the other hand, waits for only one interaction (one step) $S_{t+1}$ to form a TD target and update $V(S_t)$ using $R_{t+1}$ and $\\gamma * V(S_{t+1})$.\n",
    "\n",
    "The idea with TD is to update the $V(S_t)$ at each step.\n",
    "\n",
    "But because we didn’t experience an entire episode, we don’t have $G_t$ (expected return). Instead, we estimate $G_t$ by adding $R_{t+1}$ and the discounted value of the next state. \n",
    "\n",
    "This is called **bootstrapping**. It’s called this because TD bases its update in part on an existing estimate $V(S_{t+1})$ and not a complete sample $G_t$.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1.jpg\" width=500>\n",
    "\n",
    "This method is called TD(0) or one-step TD (update the value function after any individual step).\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1p.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d40a9-b6f4-4c43-acf5-15ae64825c82",
   "metadata": {},
   "source": [
    "# Introducing Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaf439a-2a69-4cc9-a5b2-54945762ba2a",
   "metadata": {},
   "source": [
    "## What is Q-Learning?\n",
    "\n",
    "Q-Learning is an off-policy value-based method that uses a TD approach to train its action-value function:\n",
    "- Off-policy: we’ll talk about that at the end of this unit.\n",
    "- Value-based method: finds the optimal policy indirectly by training a value or action-value function that will tell us the value of each state or each state-action pair.\n",
    "- TD approach: updates its action-value function at each step instead of at the end of the episode.\n",
    "\n",
    "Q-Learning is the algorithm we use to train our Q-function, an action-value function that determines the value of being at a particular state and taking a specific action at that state.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function.jpg\" width=500>\n",
    "\n",
    "> Given a state and action, our Q Function outputs a state-action value (also called Q-value)\n",
    "\n",
    "The Q comes from \"the Quality\" (the value) of that action at that state.\n",
    "\n",
    "Let’s recap the difference between value and reward:\n",
    "\n",
    "- The value of a state, or a state-action pair is the expected cumulative reward our agent gets if it starts at this state (or state-action pair) and then acts accordingly to its policy.\n",
    "- The reward is the feedback I get from the environment after performing an action at a state.\n",
    "\n",
    "Internally, our Q-function is encoded by a Q-table, a table where each cell corresponds to a state-action pair value. Think of this Q-table as the memory or cheat sheet of our Q-function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def85708-0e9f-41f3-af2e-81111c6fc4b2",
   "metadata": {},
   "source": [
    "If we recap, Q-Learning is the RL algorithm that:\n",
    "\n",
    "- Trains a Q-function (an action-value function), which internally is a Q-table that contains all the state-action pair values.\n",
    "- Given a state and action, our Q-function will search its Q-table for the corresponding value.\n",
    "- When the training is done, we have an optimal Q-function, which means we have optimal Q-table.\n",
    "- And if we have an optimal Q-function, we have an optimal policy since we know the best action to take at each state.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-1.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3eac25-2bfc-4425-94be-e58c05058ff9",
   "metadata": {},
   "source": [
    "## The Q-Learning algorithm\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc8352f-afe0-44ad-bb6d-0be0e1787d18",
   "metadata": {},
   "source": [
    "### Step 1: We initialize the Q-table\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-3.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f82c2d-90bc-48ba-bf09-582355ac9a31",
   "metadata": {},
   "source": [
    "### Step 2: Choose an action using the epsilon-greedy strategy\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" width=500>\n",
    "\n",
    "At the beginning of the training, the probability of doing exploration will be huge since ɛ is very high, so most of the time, we’ll explore. But as the training goes on, and consequently our Q-table gets better and better in its estimations, we progressively reduce the epsilon value since we will need less and less exploration and more exploitation.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-5.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa9040-4221-4774-afd0-b30f3773ba33",
   "metadata": {},
   "source": [
    "### Step 3: Perform action At, get reward Rt+1 and next state St+1\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-6.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171cc27-b8bd-4835-9e59-e642beb306d3",
   "metadata": {},
   "source": [
    "### Step 4: Update Q(St, At)\n",
    "\n",
    "To produce our TD target, we used the immediate reward $R_{t+1}$ plus the discounted value of the next state, computed by finding the action that maximizes the current Q-function at the next state. (We call that bootstrap).\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-7.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5bf2d3-6904-4c2b-8e01-95733722b9cb",
   "metadata": {},
   "source": [
    "Therefore, our $Q(S_t, A_t)$ update formula goes like this:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-8.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba530bb-2e0b-4160-944a-eb6c38cb872a",
   "metadata": {},
   "source": [
    "This means that to update our $Q(S_t, A_t$):\n",
    "- We need $S_t, A_t, R_{t+1}, S_{t+1}$\n",
    "- To update our Q-value at a given state-action pair, we use the TD target.\n",
    "\n",
    "How do we form the TD target?\n",
    "- We obtain the reward $R_{t+1}$ after taking the action $A_t$\n",
    "- To get the best state-action pair value for the next state, we use a greedy policy to select the next best action. Note that this is not an epsilon-greedy policy, this will always take the action with the highest state-action value.\n",
    "\n",
    "Then when the update of this Q-value is done, we start in a new state and select our action using a epsilon-greedy policy again.\n",
    "\n",
    "**This is why we say that Q Learning is an off-policy algorithm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca7f879-5d32-41e9-9558-0ad419b1ac82",
   "metadata": {},
   "source": [
    "## Off-policy vs On-policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dd58ce-ff96-48aa-b3fe-737fe7a9d6cf",
   "metadata": {},
   "source": [
    "The difference is subtle:\n",
    "- Off-policy: using a different policy for acting (inference) and updating (training).\n",
    "\n",
    "For instance, with Q-Learning, the epsilon-greedy policy (acting policy), is different from the greedy policy that is used to select the best next-state action value to update our Q-value (updating policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8222191c-b915-44f2-a4f2-069c47193292",
   "metadata": {},
   "source": [
    "**Acting Policy**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-1.jpg\" width=500>\n",
    "\n",
    "\n",
    "**Updating Policy**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-2.jpg\" width=150>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa72017f-f5c7-4a37-88e6-dbd7788bb5f2",
   "metadata": {},
   "source": [
    "- On-policy: using the same policy for acting and updating.\n",
    "\n",
    "For instance, with Sarsa, another value-based algorithm, the epsilon-greedy policy selects the next state-action pair, not a greedy policy.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-3.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3854ca0-1b27-475e-bd3f-4f94548c154e",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568842bb-80f0-4781-b0d9-a7e9c7314be1",
   "metadata": {},
   "source": [
    "# Q-Learning Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c851f8-6e03-4399-aced-912237efc664",
   "metadata": {},
   "source": [
    "Q-Learning is the RL algorithm that :\n",
    "- Trains a Q-function, an action-value function encoded, in internal memory, by a Q-table containing all the state-action pair values.\n",
    "- Given a state and action, our Q-function will search its Q-table for the corresponding value.\n",
    "- When the training is done, we have an optimal Q-function, or, equivalently, an optimal Q-table.\n",
    "- And if we have an optimal Q-function, we have an optimal policy, since we know, for each state, the best action to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e5508-db80-4eca-be36-658862c1feb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
