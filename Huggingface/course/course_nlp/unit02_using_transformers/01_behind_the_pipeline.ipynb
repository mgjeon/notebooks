{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd604310-3091-499e-a05d-c0c0038bb0ed",
   "metadata": {},
   "source": [
    "# Behind the pipeline\n",
    "\n",
    "- https://huggingface.co/learn/nlp-course/chapter2/2?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a21840c-3165-45b9-82de-b3c238af7287",
   "metadata": {},
   "source": [
    "Let’s start with a complete example, taking a look at what happened behind the scenes when we executed the following code in Chapter 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962044c3-1065-4bda-b1b6-730e9f5797cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598046541213989},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9909718a-c723-4307-905f-20f4bd5df322",
   "metadata": {},
   "source": [
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704441cd-d063-4592-8490-dc516e177352",
   "metadata": {},
   "source": [
    "## Preprocessing with a tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0433e33c-515e-49ec-ad40-5fb103e3a617",
   "metadata": {},
   "source": [
    "The first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a tokenizer, which will be responsible for:\n",
    "\n",
    "- Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
    "- Mapping each token to an integer\n",
    "- Adding additional inputs that may be useful to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d322a-4d32-46f7-8164-9700b4cbfba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a11923-8a2b-479f-899a-5f852dfb31d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320fab63-ea43-4060-8d95-b43f6593b4d1",
   "metadata": {},
   "source": [
    "The output itself is a dictionary containing two keys, `input_ids` and `attention_mask`. \n",
    "\n",
    "`input_ids` contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba0ac85-6100-4e76-bd6e-5fc82393fec9",
   "metadata": {},
   "source": [
    "## Going through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a58e26-e25f-4cd3-955f-2bf0c90869c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b600785-abd9-4b4a-9a31-4ed71719cecb",
   "metadata": {},
   "source": [
    "This architecture contains only the base Transformer module: given some inputs, it outputs what we’ll call hidden states, also known as features. For each model input, we’ll retrieve a high-dimensional vector representing the **contextual understanding of that input by the Transformer model**.\n",
    "\n",
    "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
    "\n",
    "- Batch size: The number of sequences processed at a time (2 in our example).\n",
    "- Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "- Hidden size: The vector dimension of each model input.\n",
    "\n",
    "It is said to be \"high dimensional\" because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e30f5-f105-4372-946c-3b9ebe787f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.1798,  0.2333,  0.6321,  ..., -0.3017,  0.5008,  0.1481],\n",
       "         [ 0.2758,  0.6497,  0.3200,  ..., -0.0760,  0.5136,  0.1329],\n",
       "         [ 0.9046,  0.0985,  0.2950,  ...,  0.3352, -0.1407, -0.6464],\n",
       "         ...,\n",
       "         [ 0.1466,  0.5661,  0.3235,  ..., -0.3376,  0.5100, -0.0561],\n",
       "         [ 0.7500,  0.0487,  0.1738,  ...,  0.4684,  0.0030, -0.6084],\n",
       "         [ 0.0519,  0.3729,  0.5223,  ...,  0.3584,  0.6500, -0.3883]],\n",
       "\n",
       "        [[-0.2937,  0.7283, -0.1497,  ..., -0.1187, -1.0227, -0.0422],\n",
       "         [-0.2206,  0.9384, -0.0951,  ..., -0.3643, -0.6605,  0.2407],\n",
       "         [-0.1536,  0.8988, -0.0728,  ..., -0.2189, -0.8528,  0.0710],\n",
       "         ...,\n",
       "         [-0.3017,  0.9002, -0.0200,  ..., -0.1082, -0.8412, -0.0861],\n",
       "         [-0.3338,  0.9674, -0.0729,  ..., -0.1952, -0.8181, -0.0634],\n",
       "         [-0.3454,  0.8824, -0.0426,  ..., -0.0993, -0.8329, -0.1065]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d01f13c-1411-4d85-b232-7b2272311904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0485770-29f4-4f11-a24d-e20376681a95",
   "metadata": {},
   "source": [
    "### Model heads: Making sense out of numbers\n",
    "\n",
    "The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. They are usually composed of one or a few linear layers:\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf56f6-9c3c-45a0-8174-c775b13fe5d6",
   "metadata": {},
   "source": [
    "In this diagram, the model is represented by its embeddings layer and the subsequent layers. The embeddings layer converts each input ID in the tokenized input into a vector that represents the associated token. The subsequent layers manipulate those vectors using the attention mechanism to produce the final representation of the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9936d38b-5b4d-472b-8d34-3045fb470f9f",
   "metadata": {},
   "source": [
    "For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac0153-3831-4e07-8c00-3e40548a07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8577ff2e-9187-4d6e-904d-2a87118d4a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
       "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a149d19-5ec1-4683-9b3e-f86bef122fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2cbfe8-90ae-4743-adf6-b91a1c2ae82f",
   "metadata": {},
   "source": [
    "Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943e4330-099a-431b-97d2-33a0d317f038",
   "metadata": {},
   "source": [
    "## Postprocessing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61837024-e55f-42ae-b2f9-15c1e3d7a7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82697c5e-8870-4c3a-abc3-ac68e3507c48",
   "metadata": {},
   "source": [
    "Our model predicted `[-1.5607, 1.6123]` for the first sentence and `[ 4.1692, -3.3464]` for the second one.\n",
    "\n",
    "Those are not probabilities but logits, the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a SoftMax layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3388f8a5-d14c-4882-a8b2-ee5db81bd34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5981e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc7077-65a8-4420-9512-143dc8aad143",
   "metadata": {},
   "source": [
    "Now we can see that the model predicted `[0.0402, 0.9598]` for the first sentence and `[0.9995, 0.0005]` for the second one. These are recognizable probability scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965582b8-1a92-490c-a03f-c744df40b416",
   "metadata": {},
   "source": [
    "To get the labels corresponding to each position, we can inspect the id2label attribute of the model config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355da866-4b33-418d-984c-72a3ac607cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310120db-68b9-4795-986b-665d7a3fc2e8",
   "metadata": {},
   "source": [
    "Now we can conclude that the model predicted the following:\n",
    "\n",
    "- First sentence: NEGATIVE: 0.0402, POSITIVE: 0.9598\n",
    "- Second sentence: NEGATIVE: 0.9995, POSITIVE: 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c5245-2791-4c91-94bf-4372d7f17e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
