{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physical Loss Terms\n",
    "\n",
    "https://www.physicsbaseddeeplearning.org/physicalloss.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The supervised setting can quickly yield approximate solutions with a fairly simple training process. However, what’s quite sad to see here is that we only use physical models and numerical methods as an “external” tool to produce a big pile of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using physical models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a PDE for $\\mathbf{u}(\\mathbf{x},t)$ with a time evolution, \n",
    "we can typically express it in terms of a function $\\mathcal F$ of the derivatives \n",
    "of $\\mathbf{u}$ via  \n",
    "\n",
    "$$\n",
    "  \\mathbf{u}_t = \\mathcal F ( \\mathbf{u}_{x}, \\mathbf{u}_{xx}, ... \\mathbf{u}_{xx...x} ) ,\n",
    "$$\n",
    "\n",
    "where the $_{\\mathbf{x}}$ subscripts denote spatial derivatives with respect to one of the spatial dimensions\n",
    "of higher and higher order (this can of course also include mixed derivatives with respect to different axes). $\\mathbf{u}_t$ denotes the changes over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, we can approximate the unknown $\\mathbf{u}$ itself with a neural network. If the approximation, which we call $\\tilde{\\mathbf{u}}$, is accurate, the PDE should be satisfied naturally. In other words, the residual R should be equal to zero:\n",
    "\n",
    "$$\n",
    "  R = \\mathbf{u}_t - \\mathcal F ( \\mathbf{u}_{x}, \\mathbf{u}_{xx}, ... \\mathbf{u}_{xx...x} ) = 0 .\n",
    "$$\n",
    "\n",
    "We can use pre-computed solutions \n",
    "$[x_0,y_0], ...[x_n,y_n]$ for $\\mathbf{u}$ with $\\mathbf{u}(\\mathbf{x})=y$ as constraints\n",
    "in addition to the residual terms. \n",
    "This is typically important, as most practical PDEs do not have unique solutions\n",
    "unless initial and boundary conditions are specified.\n",
    "\n",
    "Now our training objective becomes\n",
    "\n",
    "$$\n",
    "\\text{arg min}_{\\theta} \\ \\sum_i \\alpha_0 \\big( f(x_i ; \\theta)-y^*_i \\big)^2 + \\alpha_1 R(x_i) ,\n",
    "$$ (physloss-training)\n",
    "\n",
    "where $\\alpha_{0,1}$ denote hyperparameters that scale the contribution of the supervised term and \n",
    "the residual term, respectively.\n",
    "\n",
    "It's important \n",
    "to remember that physical constraints in this way only represent _soft constraints_, without guarantees\n",
    "of minimizing these constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a chosen explicit representation of the target function \n",
    "    - we could set up a _spatial_ grid (or graph, or a set of sample points)\n",
    "\n",
    "- using fully-connected neural networks to represent the solution\n",
    "    - no explicit representation exists, and the NN instead receives the _spatial coordinate_ to produce the solution at a query position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant 1: Residual derivatives for explicit representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.physicsbaseddeeplearning.org/_images/physloss-overview-v1.jpg)\n",
    ">  Variant 1: the solution is represented by a chosen computational mesh, and produced by an NN. The residual is discretized there, and can be combined with supervised terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this variant 1 has a lot in common with _differentiable physics_ training (it's basically a subset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant 2: Derivatives from a neural network representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.physicsbaseddeeplearning.org/_images/physloss-overview-v2.jpg)\n",
    "> Variant 2: the solution is produced by a fully-connected network, typically requiring a supervised loss with a combination of derivatives from the neural network for the residual. These NN derivatives have their own share of advantages and disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary so far\n",
    "\n",
    "The approach above gives us a method to include physical equations into DL learning as a soft constraint: the residual loss.\n",
    "Typically, this setup is suitable for _inverse problems_, where we have certain measurements or observations\n",
    "for which we want to find a PDE solution. Because of the high cost of the reconstruction, the solution manifold shouldn't be overly complex. E.g., it is typically not possible \n",
    "to capture a wide range of solutions, such as with the previous supervised airfoil example, by only using a physical residual loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
