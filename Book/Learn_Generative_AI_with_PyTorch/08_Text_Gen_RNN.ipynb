{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Text generation with recurrent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data, Tokenization, Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'files/anna.txt'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import shutil\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "urlretrieve(\"https://raw.githubusercontent.com/LeanManager/NLP-PyTorch/refs/heads/master/data/anna.txt\", \"anna-raw.txt\")\n",
    "\n",
    "f = open('anna-raw.txt', 'r')\n",
    "g = open('anna.txt', 'w')\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    if i < 39888:\n",
    "        g.write(line)\n",
    "    i += 1\n",
    "f.close()\n",
    "g.close()\n",
    "\n",
    "os.remove('anna-raw.txt')\n",
    "os.makedirs('files', exist_ok=True)\n",
    "shutil.move('anna.txt', 'files/anna.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chapter', '1\\n\\n\\nHappy', 'families', 'are', 'all', 'alike;', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own\\nway.\\n\\nEverything', 'was', 'in', 'confusion', 'in', 'the', \"Oblonskys'\"]\n"
     ]
    }
   ],
   "source": [
    "with open(\"files/anna.txt\",\"r\") as f: \n",
    "    text=f.read() \n",
    "    words=text.split(\" \") \n",
    "    print(words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text=text.lower().replace(\"\\n\", \" \")\n",
    "clean_text=clean_text.replace(\"-\", \" \")\n",
    "for x in \",.:;?!$()/_&%*@'`\": \n",
    "    clean_text=clean_text.replace(f\"{x}\", f\" {x} \")\n",
    "clean_text=clean_text.replace('\"', ' \" ')\n",
    "text=clean_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chapter', '1', 'happy', 'families', 'are', 'all', 'alike', ';', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', '.', 'everything', 'was']\n"
     ]
    }
   ],
   "source": [
    "print(text[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'the', '\"', 'and', 'to', 'of', 'he', \"'\", 'a']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts=Counter(text) \n",
    "words=sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the text contains 437098 words\n",
      "there are 12778 unique tokens\n"
     ]
    }
   ],
   "source": [
    "num_unique_words=len(words)\n",
    "text_length=len(text)\n",
    "print(f\"the text contains {text_length} words\") \n",
    "print(f\"there are {num_unique_words} unique tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{',': 0, '.': 1, 'the': 2, '\"': 3, 'and': 4, 'to': 5, 'of': 6, 'he': 7, \"'\": 8, 'a': 9}\n",
      "{0: ',', 1: '.', 2: 'the', 3: '\"', 4: 'and', 5: 'to', 6: 'of', 7: 'he', 8: \"'\", 9: 'a'}\n"
     ]
    }
   ],
   "source": [
    "word_to_int={v:k for k,v in enumerate(words)}\n",
    "int_to_word={k:v for k,v in enumerate(words)}\n",
    "print({k:v for k,v in word_to_int.items() if k in words[:10]})\n",
    "print({k:v for k,v in int_to_word.items() if v in words[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chapter', '1', 'happy', 'families', 'are', 'all', 'alike', ';', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', '.', 'everything', 'was']\n",
      "[208, 2755, 280, 2981, 83, 31, 2419, 35, 202, 685, 362, 38, 685, 10, 236, 147, 166, 1, 149, 12]\n"
     ]
    }
   ],
   "source": [
    "print(text[0:20]) \n",
    "wordidx=[word_to_int[w] for w in text] \n",
    "print([word_to_int[w] for w in text[0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "seq_len=100 \n",
    "xys=[] \n",
    "for n in range(0, len(wordidx)-seq_len-1):\n",
    "    x = wordidx[n:n+seq_len] \n",
    "    y = wordidx[n+1:n+seq_len+1]\n",
    "    xys.append((torch.tensor(x),(torch.tensor(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "torch.manual_seed(42) \n",
    "batch_size=32 \n",
    "loader = DataLoader(xys, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "class WordLSTM(nn.Module): \n",
    "    def __init__(self, input_size=128, n_embed=128, n_layers=3, drop_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size \n",
    "        self.drop_prob = drop_prob \n",
    "        self.n_layers = n_layers \n",
    "        self.n_embed = n_embed\n",
    "        vocab_size = len(word_to_int)\n",
    "        self.embedding = nn.Embedding(vocab_size,n_embed)\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size, \n",
    "                            hidden_size=self.n_embed, \n",
    "                            num_layers=self.n_layers, \n",
    "                            dropout=self.drop_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(input_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hc): \n",
    "        embed=self.embedding(x) \n",
    "        x, hc = self.lstm(embed, hc) \n",
    "        x = self.fc(x) \n",
    "        return x, hc\n",
    "    \n",
    "    def init_hidden(self, n_seqs):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, \n",
    "                           n_seqs, self.n_embed).zero_(), \n",
    "                weight.new(self.n_layers, \n",
    "                           n_seqs, self.n_embed).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=WordLSTM().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001 \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 999/13657 [00:36<07:46, 27.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 0 iteration 1000\\ average loss = 6.397078318119049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train() \n",
    "\n",
    "for epoch in range(50): \n",
    "    tloss=0\n",
    "    sh,sc = model.init_hidden(batch_size)\n",
    "    for i, (x,y) in tqdm(enumerate(loader), total=len(loader)):\n",
    "        if x.shape[0]==batch_size:\n",
    "            inputs, targets = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, (sh,sc) = model(inputs, (sh,sc))\n",
    "            # output : (batch_size, seq_len, vocab_size)\n",
    "            #       -> (batch_size, vocab_size, seq_len)\n",
    "            # targets: (batch_size, seq_len)\n",
    "            loss = loss_func(output.transpose(1,2),targets)\n",
    "            sh,sc=sh.detach(),sc.detach()\n",
    "            loss.backward() \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optimizer.step() \n",
    "            tloss+=loss.item()\n",
    "        if (i+1)%1000==0: \n",
    "            print(f\"at epoch {epoch} iteration {i+1}\\ average loss = {tloss/(i+1)}\")\n",
    "            if (i+1) == 1000:\n",
    "                break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(),\"files/wordLSTM.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# with open(\"files/word_to_int.p\",\"wb\") as fb: \n",
    "#     pickle.dump(word_to_int, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlretrieve(\"https://mng.bz/vJZa\", \"wordLSTM.zip\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"wordLSTM.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"files/\")\n",
    "os.remove(\"wordLSTM.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlretrieve(\"https://github.com/markhliu/DGAI/raw/refs/heads/main/files/word_to_int.p\", \"files/word_to_int.p\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"files/wordLSTM.pth\", map_location=device, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"files/word_to_int.p\",\"rb\") as fb:\n",
    "    word_to_int = pickle.load(fb) \n",
    "int_to_word={v:k for k,v in word_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def sample(model, prompt, length=200): \n",
    "    model.eval() \n",
    "    text = prompt.lower().split(' ') \n",
    "    hc = model.init_hidden(1) \n",
    "    length = length - len(text) \n",
    "    for i in range(0, length): \n",
    "        if len(text)<= seq_len: \n",
    "            x = torch.tensor([[word_to_int[w] for w in text]]) \n",
    "        else: \n",
    "            x = torch.tensor([[word_to_int[w] for w in text[-seq_len:]]])\n",
    "        inputs = x.to(device)\n",
    "        output, hc = model(inputs, hc)\n",
    "        # torch.Size([1, 100, 12778])\n",
    "        logits = output[0][-1] # torch.Size([12778])\n",
    "        p = nn.functional.softmax(logits, dim=0).detach().cpu().numpy()\n",
    "        idx = np.random.choice(len(logits), p=p)\n",
    "        text.append(int_to_word[idx])\n",
    "    text=\" \".join(text) \n",
    "    for m in \",.:;?!$()/_&%*@'`\": \n",
    "        text=text.replace(f\" {m}\", f\"{m} \") \n",
    "    text=text.replace('\" ', '\"') \n",
    "    text=text.replace(\"' \", \"'\") \n",
    "    text=text.replace('\" ', '\"') \n",
    "    text=text.replace(\"' \", \"'\") \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anna and the prince did not forget what he had not spoken.  when the softening barrier was not so long as he had talked to his brother,  all the hopelessness of the impression.  \"official tail,  a man who had tried him,  nothing with his own satisfaction.  \"truly!  every sin has occurred,  and he would have thought with that words.  \"yes,  disgusting!  \"the recollection of his confusion,  he said:  \"but he's better as merely old,  wide patches,  sat down with a long while,  fresh,  and attired in it,  her head and with its skin step in its place off.  cord along the carriage,  as though wishing to send it,  he had invited to tell him.  next confession he did not like to ask him.  he wrote that the business whether he had reached a thousand many hundred roubles,  a foreign petersburg army,  not especially,  this was agafea mihalovna's insight,  but he longed it in his soul that his wife's behavior would not be\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42) \n",
    "np.random.seed(42) \n",
    "print(sample(model, prompt='Anna and the prince'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt, length=200, top_k=None, temperature=1):\n",
    "    model.eval() \n",
    "    text = prompt.lower().split(' ') \n",
    "    hc = model.init_hidden(1) \n",
    "    length = length - len(text) \n",
    "    for i in range(0, length): \n",
    "        if len(text)<= seq_len: \n",
    "            x = torch.tensor([[word_to_int[w] for w in text]]) \n",
    "        else: \n",
    "            x = torch.tensor([[word_to_int[w] for w in text[-seq_len:]]])\n",
    "        inputs = x.to(device)\n",
    "        output, hc = model(inputs, hc)\n",
    "        # torch.Size([1, 100, 12778])\n",
    "        logits = output[0][-1] # torch.Size([12778])\n",
    "        logits = logits/temperature\n",
    "        p = nn.functional.softmax(logits, dim=0).detach().cpu()\n",
    "        if top_k is None: \n",
    "            idx = np.random.choice(len(logits), p=p.numpy()) \n",
    "        else: \n",
    "            ps, tops = p.topk(top_k) \n",
    "            ps=ps/ps.sum() \n",
    "            idx = np.random.choice(tops, p=ps.numpy())\n",
    "        text.append(int_to_word[idx])\n",
    "    text=\" \".join(text) \n",
    "    for m in \",.:;?!$()/_&%*@'`\": \n",
    "        text=text.replace(f\" {m}\", f\"{m} \") \n",
    "    text=text.replace('\" ', '\"') \n",
    "    text=text.replace(\"' \", \"'\") \n",
    "    text=text.replace('\" ', '\"') \n",
    "    text=text.replace(\"' \", \"'\") \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm not going to see you\n",
      "i'm not going to see those\n",
      "i'm not going to see me\n",
      "i'm not going to see you\n",
      "i'm not going to see her\n",
      "i'm not going to see her\n",
      "i'm not going to see the\n",
      "i'm not going to see my\n",
      "i'm not going to see you\n",
      "i'm not going to see me\n"
     ]
    }
   ],
   "source": [
    "prompt=\"I ' m not going to see\"\n",
    "torch.manual_seed(42) \n",
    "np.random.seed(42) \n",
    "for _ in range(10): \n",
    "    print(generate(model, prompt, length=len(prompt.split(\" \"))+1, \n",
    "                   top_k=None, temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm not going to see you\n",
      "i'm not going to see the\n",
      "i'm not going to see her\n",
      "i'm not going to see you\n",
      "i'm not going to see you\n",
      "i'm not going to see you\n",
      "i'm not going to see you\n",
      "i'm not going to see her\n",
      "i'm not going to see you\n",
      "i'm not going to see her\n"
     ]
    }
   ],
   "source": [
    "prompt=\"I ' m not going to see\"\n",
    "torch.manual_seed(42) \n",
    "np.random.seed(42) \n",
    "for _ in range(10): \n",
    "    print(generate(model, prompt, length=len(prompt.split(\" \"))+1, \n",
    "                   top_k=3, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm not going to see them\n",
      "i'm not going to see scarlatina\n",
      "i'm not going to see behind\n",
      "i'm not going to see us\n",
      "i'm not going to see it\n",
      "i'm not going to see it\n",
      "i'm not going to see a\n",
      "i'm not going to see misery\n",
      "i'm not going to see another\n",
      "i'm not going to see seryozha\n"
     ]
    }
   ],
   "source": [
    "prompt=\"I ' m not going to see\"\n",
    "torch.manual_seed(42) \n",
    "np.random.seed(42) \n",
    "for _ in range(10): \n",
    "    print(generate(model, prompt, length=len(prompt.split(\" \"))+1, \n",
    "                   top_k=None, temperature=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
