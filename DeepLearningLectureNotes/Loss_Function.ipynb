{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Understanding Deep Learning; Chapter 5](https://udlbook.github.io/udlbook/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train these models, we seek the parameters that produce the best possible mapping from input to output for the task we are considering.\n",
    "\n",
    "We have a training dataset $\\{x_i, y_i\\}_{i=1}^{N}$ of input/output pairs.\n",
    "Consider a model $f_{\\theta}(x)$ with parameters $\\theta$ that computes an output from input $x$.\n",
    "\n",
    "We often think that the model directly computes a prediction $y$. We now shift perspective and consider the model as computing a conditional probability distribution $p(y|x)$ over possible outputs $y$ given input $x$.\n",
    "\n",
    "A **loss function** $L(\\theta)$ returns a single number that describes the mismatch between the model predictions and their corresponding ground-truth outputs.\n",
    "The loss encourages each training output $y_i$ to have a high probability under the distribution $p(y_i|x_i)$ computed from the corresponding input $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative log-likelihood criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a parametric distribution $p(y|\\phi)$ defined on the output domain $y$. Then we use the neural network to compute one or more of the parameters $\\phi$ of this distribution.\n",
    "\n",
    "The model now computes different distribution parameters $\\phi_i = f_\\theta(x_i)$ for each training input $x_i$. Each observed training output $y_i$ should have high probability under its corresponding distribution $p(y_i|\\phi_i)$. Hence, we choose the model parameters $\\theta$ so that they maximize the combined probability across all $N$ training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A conditional probability $p(z|\\psi)$ can be considered in two ways.\n",
    "- As a function of $z$, it is a probability distribution that sums to one.\n",
    "- As a function of $\\psi$, it is a likelihood and does not generally sum to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that\n",
    "- the data are identically distributed (the form of the probability distribution over the outputs $y_i$ is the same for each data point).\n",
    "- the conditional distribution $p(y_i|x_i)$ of the output given the input are independent, so the total likelihood of the training data decomposes as:\n",
    "$$\n",
    "p(y_1, y_2, ..., y_N | x_1, x_2, ..., x_N) = \\prod_{i=1}^{N}p(y_i|x_i)\n",
    "$$\n",
    "In other words, we assume the data are **independent and identically distributed (i.i.d.)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the model parameter $\\hat{\\theta}$ we want to find is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\theta} & = \\underset{\\theta}{\\mathrm{argmax}}\\left[ \\prod_{i=1}^{N} p(y_i|x_i)  \\right] \\\\\n",
    "& = \\underset{\\theta}{\\mathrm{argmax}}\\left[ \\prod_{i=1}^{N} p(y_i|\\phi_i)  \\right] \\\\\n",
    "& = \\underset{\\theta}{\\mathrm{argmax}}\\left[ \\prod_{i=1}^{N} p(y_i|f_\\theta(x_i))  \\right] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The combined probability term is the **likelihood** of the parameters and this equation is known as the **maximum likelihood** criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum likelihood criterion is not very practical. Each term $p(y_i|f_\\theta(x_i))$ can be small, so the product of many of these terms can be tiny. It may be difficult to represent this quantity with finite precision arithmetic. Fortunately, we can equivalently maximize the logarithm of the likelihood:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\theta} & = \\underset{\\theta}{\\mathrm{argmax}}\\left[ \\prod_{i=1}^{N} p(y_i|f_\\theta(x_i))  \\right] \\\\\n",
    "& = \\underset{\\theta}{\\mathrm{argmax}}\\left[ \\log \\prod_{i=1}^{N} p(y_i|f_\\theta(x_i))  \\right] \\\\\n",
    "& = \\underset{\\theta}{\\mathrm{argmax}}\\left[ \\sum_{i=1}^{N} \\log p(y_i|f_\\theta(x_i))  \\right] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This **log-likelihood** criterion is equivalent because the logarithm is a monotonically increasing function. The log-likelihood criterion has the practical advantage of using a sum of terms, not a product, so representing it with finite precision isn't problematic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention, model fitting problems are framed in terms of minimizing a loss. To convert the maximum log-likelihood criterion to a minimization\n",
    "problem, we multiply by minus one, which gives us the **negative log-likelihood criterion**:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\theta} & = \\underset{\\theta}{\\mathrm{argmin}}\\left[-\\sum_{i=1}^{N}\\log p(y_i|f_\\theta(x_i))\\right] \\\\\n",
    "& = \\underset{\\theta}{\\mathrm{argmin}}[L(\\theta)]\n",
    "\\end{align}\n",
    "$$\n",
    "which is what forms the final loss function $L(\\theta)$.\n",
    "\n",
    "$$\n",
    "L(\\theta) = -\\sum_{i=1}^{N}\\log p(y_i|f_\\theta(x_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network no longer directly predicts the outputs $y$ but instead determines a probability distribution over $y$. When we perform inference, we often want a point estimate rather than a distribution, so we return the maximum of the distribution:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\underset{y}{\\mathrm{argmax}}[p(y|f_{\\hat{\\theta}}(x))]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Recipe for constructing loss functions\n",
    "\n",
    "The recipe for constructing loss functions for training data $\\{x_i, y_i\\}_{i=1}^{N}$ using the maximum likelihood approach is:\n",
    "\n",
    "1. Choose a sutiable probability distribution $p(y|\\phi)$ defined over the domain of the predictions $y$ with distribution parameters $\\phi$.\n",
    "1. Set the machine learning model $f_\\theta(x)$ to predict one or more of these parameters, so $\\phi=f_\\theta(x)$ and $p(y|\\phi)=p(y|f_\\theta(x))$.\n",
    "1. To train the model, find the network parameters $\\hat{\\theta}$ that minimize the negative log-likelihood loss function over the training dataset pairs $\\{x_i, y_i\\}_{i=1}^{N}$:\n",
    "$$\n",
    "\\hat{\\theta} = \\underset{\\theta}{\\mathrm{argmin}}[L(\\theta)] = \\underset{\\theta}{\\mathrm{argmin}}\\left[-\\sum_{i=1}^{N}\\log p(y_i|f_\\theta(x_i))\\right]\n",
    "$$\n",
    "1. To perform inference for a new test example $x$, return either the full distribution $p(y|f_{\\hat{\\theta}}(x))$ or the value where the distribution is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy\n",
    "\n",
    "The **information** quantifies the number of bits required to encode and transmit an event. Lower probability events have more information, higher probability events have less information.\n",
    "\n",
    "In information theory, we like to describe the “surprise” of an event. An event is more surprising the less likely it is, meaning it contains more information.\n",
    "\n",
    "- Low Probability Event (surprising): More information.\n",
    "- Higher Probability Event (unsurprising): Less information.\n",
    "\n",
    "Information $h(x)$ can be calculated for an event $x$, given the probability of the event $P(x)$ as follows:\n",
    "\n",
    "$$\n",
    "h(x) = -\\log P(x)\n",
    "$$\n",
    "\n",
    "The **entropy** is the number of bits required to transmit a randomly selected event from a probability distribution. A skewed distribution has a low entropy, whereas a distribution where events have equal probability has a larger entropy.\n",
    "\n",
    "A skewed probability distribution has less “surprise” and in turn a low entropy because likely events dominate. Balanced distribution are more surprising and turn have higher entropy because events are equally likely.\n",
    "\n",
    "- Skewed Probability Distribution (unsurprising): Low entropy.\n",
    "- Balanced Probability Distribution (surprising): High entropy.\n",
    "\n",
    "Entropy $H(P)$ is an expected information for probability distribution $P(x)$.\n",
    "\n",
    "$$\n",
    "H(P) = \\sum_x P(x)h(x) = -\\sum_x P(x) \\log P(x)\n",
    "$$\n",
    "\n",
    "The **cross-entropy** is the average number of bits needed to encode data coming from a source with distribution $P$ when we use model $Q$\n",
    "\n",
    "$$\n",
    "H(P, Q) = -\\sum_x P(x) \\log Q(x)\n",
    "$$\n",
    "\n",
    "The **Kullback-Leibler (KL) divergence** is the average number of extra bits needed to encode the data, due to the fact that we used distribution $Q$ to encode the data instead of the true distribution $P$.\n",
    "\n",
    "- Cross-Entropy: Average number of total bits to represent an event from $Q$ instead of $P$.\n",
    "- KL Divergence (also called Relative Entropy): Average number of extra bits to represent an event from $Q$ instead of $P$.\n",
    "\n",
    "$$\n",
    "D_{KL}(P || Q) = H(P, Q) - H(P) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy criterion\n",
    "\n",
    "The cross-entropy loss is based on the idea of finding parameters $\\phi$ that minimize the distance between the empirical distribution $q(y)$ of the observed data $y$ and a model distribution $p(y|\\phi)$.\n",
    "\n",
    "Consider an empirical data distribution at points $\\{ y_i \\}_{i=1}^{N}$. We can describe this as a weighted sum of point masses:\n",
    "\n",
    "$$\n",
    "q(y) = \\frac{1}{N} \\sum_{i=1}^{N} \\delta(y-y_i)\n",
    "$$\n",
    "\n",
    "where $\\delta$ is the Dirac delta function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance between two probability distributions $q(z)$ and $p(z)$ can be evaluated using the Kullback-Leiber (KL) divergence:\n",
    "\n",
    "$$\n",
    "D_{KL}(q||p) = \\int_{-\\infty}^{\\infty} q(z) \\log \\frac{q(z)}{p(z)} dz\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize the KL divergence between this empirical distribution $q(y)$ and the model distribution $p(y|\\phi)$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\phi} & = \\underset{\\phi}{\\mathrm{argmin}}\\left[\n",
    "  \\int_{-\\infty}^{\\infty} q(y) \\log q(y) dy - \\int_{-\\infty}^{\\infty} q(y) \\log p(y|\\phi) dy\n",
    "  \\right] \\\\\n",
    "  & = \\underset{\\phi}{\\mathrm{argmin}}\\left[- \\int_{-\\infty}^{\\infty} q(y) \\log p(y|\\phi) dy\n",
    "  \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the first term disapperas, as it has no dependence on $\\phi$.  The remaining second term is known as the **cross-entropy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\phi}\n",
    "  & = \\underset{\\phi}{\\mathrm{argmin}}\\left[- \\int_{-\\infty}^{\\infty} \\frac{1}{N} \\sum_{i=1}^{N} \\delta(y-y_i) \\log p(y|\\phi) dy\n",
    "  \\right] \\\\\n",
    "  & = \\underset{\\phi}{\\mathrm{argmin}}\\left[- \\frac{1}{N} \\sum_{i=1}^{N} \\log p(y_i|\\phi)\n",
    "  \\right] \\\\\n",
    "  & = \\underset{\\phi}{\\mathrm{argmin}}\\left[- \\sum_{i=1}^{N} \\log p(y_i|\\phi)\n",
    "  \\right]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In machine learning, the distribution parameters $\\phi$ are computed by the model $f_\\theta(x_i)$, so we have:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\underset{\\theta}{\\mathrm{argmin}}\\left[- \\sum_{i=1}^{N} \\log p(y_i|f_\\theta(x_i))\n",
    "  \\right]\n",
    "$$\n",
    "\n",
    "This is precisely the negative log-likelihood criterion.\n",
    "\n",
    "**It follows that the negative log-likelihood criterion (from maximizing the data likelihood) and the cross-entropy criterion (from minimizing the distance between the model and empirical data distributions) are equivalent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
