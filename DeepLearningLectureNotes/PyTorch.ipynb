{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\vec{a} = \\begin{bmatrix}\n",
    "           a_{1} \\\\\n",
    "           a_{2} \\\\\n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec{b} = \\begin{bmatrix}\n",
    "           b_{1} \\\\\n",
    "           b_{2} \\\\\n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "L = \\sum_{i=1}^{2} (3a_i^3 - b_i^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\vec{a}} = \\left[ \\frac{\\partial L}{\\partial a_1} ; \\frac{\\partial L}{\\partial a_2}\\right] = \\left[ 9a_1^2 ; 9a_2^2 \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\vec{b}} = \\left[ \\frac{\\partial L}{\\partial b_1} ; \\frac{\\partial L}{\\partial b_2}\\right] = \\left[ -2b_1 ; -2b_2 \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "If $\\vec{a}^T=[2 ; 3]$ and $\\vec{b}^T=[6 ; 4]$, then\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\vec{a}} = \\left[ 36;81 \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\vec{b}} = \\left[ -12;-8 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLda = lambda a: torch.Tensor([9*a[0]**2, 9*a[1]**2])\n",
    "dLdb = lambda b: torch.Tensor([-2*b[0], -2*b[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(53.)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([2., 3.])\n",
    "b = torch.tensor([6., 4.])\n",
    "\n",
    "L = torch.sum((3*a**3 - b**2))\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    L.backward()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(a.requires_grad)\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.requires_grad = True\n",
    "b.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(a.requires_grad)\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(53., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = torch.sum((3*a**3 - b**2))\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36., 81.])\n",
      "tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36., 81.])\n",
      "tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "print(dLda(a))\n",
    "print(dLdb(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = a.clone()\n",
    "b0 = b.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.005\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.005\n",
    "optimizer = SGD([a, b], lr=lr)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(53., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.sum((3*a**3 - b**2))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36., 81.])\n",
      "tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.], requires_grad=True)\n",
      "tensor([6., 4.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8200, 2.5950], requires_grad=True)\n",
      "tensor([6.0600, 4.0400], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.optim.SGD.html\n",
    "\n",
    "for t=1 to ... do\n",
    "$$\n",
    "g_t \\leftarrow \\nabla_\\theta f_t(\\theta_{t-1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8200, 2.5950], requires_grad=True)\n",
      "tensor([1.8200, 2.5950], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(a0-lr*a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.0600, 4.0400], requires_grad=True)\n",
      "tensor([6.0600, 4.0400], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(b)\n",
    "print(b0-lr*b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.4649, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.sum((3*a**3 - b**2))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 65.8116, 141.6062])\n",
      "tensor([-24.1200, -16.0800])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([29.8116, 60.6062])\n",
      "tensor([-12.1200,  -8.0800])\n"
     ]
    }
   ],
   "source": [
    "print(dLda(a))\n",
    "print(dLdb(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([29.8116, 60.6062])\n",
      "tensor([-12.1200,  -8.0800])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad - dLda(a0))\n",
    "print(b.grad - dLdb(b0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html\n",
    "\n",
    "This function accumulates gradients in the leaves - you might need to zero `.grad` attributes or set them to `None` before calling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "None\n",
      "None\n",
      "tensor(53., grad_fn=<SumBackward0>)\n",
      "\n",
      "tensor([36., 81.])\n",
      "tensor([-12.,  -8.])\n",
      "\n",
      "tensor([36., 81.])\n",
      "tensor([-12.,  -8.])\n",
      "===============\n",
      "tensor([1.8200, 2.5950], requires_grad=True)\n",
      "tensor([6.0600, 4.0400], requires_grad=True)\n",
      "===============\n",
      "None\n",
      "None\n",
      "===============\n",
      "tensor(17.4649, grad_fn=<SumBackward0>)\n",
      "\n",
      "tensor([29.8116, 60.6062])\n",
      "tensor([-12.1200,  -8.0800])\n",
      "\n",
      "tensor([29.8116, 60.6062])\n",
      "tensor([-12.1200,  -8.0800])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "\n",
    "lr = 0.005\n",
    "optimizer = SGD([a, b], lr=lr)\n",
    "\n",
    "print('===============')\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "L = torch.sum((3*a**3 - b**2))\n",
    "L.backward()\n",
    "print(L)\n",
    "print()\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "print()\n",
    "print(dLda(a))\n",
    "print(dLdb(b))\n",
    "\n",
    "print('===============')\n",
    "optimizer.step()\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "print('===============')\n",
    "optimizer.zero_grad()\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "\n",
    "print('===============')\n",
    "L = torch.sum((3*a**3 - b**2))\n",
    "L.backward()\n",
    "print(L)\n",
    "print()\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "print()\n",
    "print(dLda(a))\n",
    "print(dLdb(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.464889526367188"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(L.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.1\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "\n",
    "a0 = a.clone()\n",
    "b0 = b.clone()\n",
    "\n",
    "lr = 0.1\n",
    "optimizer = Adam([a, b], lr=lr)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(53., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.sum((3*a**3 - b**2))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36., 81.])\n",
      "tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.], requires_grad=True)\n",
      "tensor([6., 4.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9000, 2.9000], requires_grad=True)\n",
      "tensor([6.1000, 4.1000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
    "\n",
    "for t=1 to ... do\n",
    "$$\n",
    "g_t \\leftarrow \\nabla_\\theta f_t(\\theta_t-1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "m_t \\leftarrow \\beta_1 m_{t-1} + (1-\\beta_1)g_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_t \\leftarrow \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\widehat{m_t} \\leftarrow m_t/(1-\\beta_1^t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\widehat{v_t} \\leftarrow v_t/(1-\\beta_2^t)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/(\\sqrt{\\widehat{v_t}}+\\epsilon)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9000, 2.9000], grad_fn=<SubBackward0>)\n",
      "tensor([1.9000, 2.9000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-08\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "\n",
    "g1 = a.grad.clone()\n",
    "m0 = 0\n",
    "v0 = 0\n",
    "m1 = beta1*m0 + (1-beta1)*g1\n",
    "v1 = beta2*v0 + (1-beta2)*g1**2\n",
    "m1_hat = m1/(1-beta1**1)\n",
    "v1_hat = v1/(1-beta2**1)\n",
    "\n",
    "print(a0 - lr*m1_hat/(torch.sqrt(v1_hat)+eps))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36., 81.])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(39.7240, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.sum((3*a**3 - b**2))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32.4900, 75.6900])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9000, 2.9000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = a.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8004, 2.8002], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8004, 2.8002], grad_fn=<SubBackward0>)\n",
      "tensor([1.8004, 2.8002], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "g2 = a.grad.clone()\n",
    "m2 = beta1*m1 + (1-beta1)*g2\n",
    "v2 = beta2*v1 + (1-beta2)*g2**2\n",
    "m2_hat = m2/(1-beta1**2)\n",
    "v2_hat = v2/(1-beta2**2)\n",
    "\n",
    "print(a1 - lr*m2_hat/(torch.sqrt(v2_hat)+eps))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
