{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "- https://d2l.ai/chapter_convolutional-neural-networks/index.html\n",
    "- https://github.com/fastai/course22p2/blob/master/nbs/07_convolutions.ipynb\n",
    "- https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MLP is one of appropriate options when we are dealing with tabular data. By tabular, we mean that the data consist of rows corresponding to examples and columns corresponding to features. With tabular data, we might anticipate that the patterns we seek could involve interactions among the features, but we do not assume any structure a priori concerning how the features interact.\n",
    "\n",
    "Sometimes, we truly lack the knowledge to be able to guide the construction of fancier architectures. In these cases, an MLP may be the best that we can do. However, for high-dimensional perceptual data, such structureless networks can grow unwieldy.\n",
    "\n",
    "For instance, let's consider an example of distinguishing cats from dogs. Say that we do a thorough job in data collection, collecting an annotated dataset of one-megapixel photographs. This means that each input to the network has one million dimensions. Even an aggressive reduction to one thousand hidden dimensions would require a fully connected layer characterized by $10^6 \\times 10^3 = 10^9$ parameters. Unless we have lots of GPUs, a talent for distributed optimization, and an extraordinary amount of patience, learning the parameters of this network may turn out to be infeasible.\n",
    "\n",
    "Images exhibit rich structure that can be exploited by humans and machine learning models alike. **Convolutional neural networks (CNNs)** are one creative way that machine learning has embraced for exploiting some of the known structure in natural images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invariance\n",
    "\n",
    "Imagine that we want to detect an object in an image. It seems reasonable that whatever method we use to recognize objects should not be overly concerned with the precise location of the object in the image. We could sweep the image with an object detector that could assign a score to each patch, indicating the likelihood that the patch contains the object we find.\n",
    "\n",
    "CNNs systematize this idea of **spatial invariance**, exploiting it to learn useful representations with fewer parameters.\n",
    "\n",
    "We can now make these intuitions more concrete by enumerating a few desiderata to guide our design of a neural network architecture suitable for computer vision:\n",
    "1. In the earliest layers, our network should respond similarly to the same patch, regardless of where it appears in the image. This principle is called **translation invariance**.\n",
    "1. The earliest layers of the network should focus on local regions, without regard for the contents of the image in distant regions. This is the **locality principle**. Eventually, these local representations can be aggregated to make predictions at the whole image level.\n",
    "1. As we proceed, deeper layers should be able to capture longer-range features of the image, in a way similar to higher level vision in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraining the MLP\n",
    "\n",
    "To start off, we can consider an MLP\n",
    "with two-dimensional images $\\mathbf{X}$ as inputs\n",
    "and their immediate hidden representations\n",
    "$\\mathbf{H}$ similarly represented as matrices, where both $\\mathbf{X}$ and $\\mathbf{H}$ have the same shape. We now imagine that not only the inputs but\n",
    "also the hidden representations possess spatial structure.\n",
    "\n",
    "Let $[\\mathbf{X}]_{i, j}$ and $[\\mathbf{H}]_{i, j}$ denote the pixel\n",
    "at location $(i,j)$\n",
    "in the input image and hidden representation, respectively.\n",
    "We could formally express the fully connected layer as\n",
    "\n",
    "$$\\begin{aligned} \\left[\\mathbf{H}\\right]_{i, j} &= [\\mathbf{U}]_{i, j} + \\sum_k \\sum_l[\\mathsf{W}]_{i, j, k, l}  [\\mathbf{X}]_{k, l}\\\\ &=  [\\mathbf{U}]_{i, j} +\n",
    "\\sum_a \\sum_b [\\mathsf{V}]_{i, j, a, b}  [\\mathbf{X}]_{i+a, j+b}.\\end{aligned}$$\n",
    "\n",
    "The switch from $\\mathsf{W}$ to $\\mathsf{V}$ is entirely cosmetic for now\n",
    "since there is a one-to-one correspondence\n",
    "between coefficients in both fourth-order tensors.\n",
    "We simply re-index the subscripts $(k, l)$\n",
    "such that $k = i+a$ and $l = j+b$.\n",
    "In other words, we set $[\\mathsf{V}]_{i, j, a, b} = [\\mathsf{W}]_{i, j, i+a, j+b}$.\n",
    "The indices $a$ and $b$ run over both positive and negative offsets,\n",
    "covering the entire image.\n",
    "For any given location ($i$, $j$) in the hidden representation $[\\mathbf{H}]_{i, j}$,\n",
    "we compute its value by summing over pixels in $x$,\n",
    "centered around $(i, j)$ and weighted by $[\\mathsf{V}]_{i, j, a, b}$. Before we carry on, let's consider the total number of parameters required for a *single* layer in this parametrization: a $1000 \\times 1000$ image (1 megapixel) is mapped to a $1000 \\times 1000$ hidden representation. This requires $10^{12}$ parameters, far beyond what computers currently can handle.  \n",
    "\n",
    "### Translation Invariance\n",
    "\n",
    "Now let's invoke the first principle\n",
    "established above: translation invariance.\n",
    "This implies that a shift in the input $\\mathbf{X}$\n",
    "should simply lead to a shift in the hidden representation $\\mathbf{H}$.\n",
    "This is only possible if $\\mathsf{V}$ and $\\mathbf{U}$ do not actually depend on $(i, j)$. As such,\n",
    "we have $[\\mathsf{V}]_{i, j, a, b} = [\\mathbf{V}]_{a, b}$ and $\\mathbf{U}$ is a constant, say $u$.\n",
    "As a result, we can simplify the definition for $\\mathbf{H}$:\n",
    "\n",
    "$$[\\mathbf{H}]_{i, j} = u + \\sum_a\\sum_b [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$\n",
    "\n",
    "\n",
    "This is a *convolution* (actually cross-correlation)!\n",
    "We are effectively weighting pixels at $(i+a, j+b)$\n",
    "in the vicinity of location $(i, j)$ with coefficients $[\\mathbf{V}]_{a, b}$\n",
    "to obtain the value $[\\mathbf{H}]_{i, j}$.\n",
    "Note that $[\\mathbf{V}]_{a, b}$ needs many fewer coefficients than $[\\mathsf{V}]_{i, j, a, b}$ since it\n",
    "no longer depends on the location within the image. Consequently, the number of parameters required is no longer $10^{12}$ but a much more reasonable $4 \\times 10^6$: we still have the dependency on $a, b \\in (-1000, 1000)$.\n",
    "\n",
    "###  Locality\n",
    "\n",
    "Now let's invoke the second principle: locality.\n",
    "As motivated above, we believe that we should not have\n",
    "to look very far away from location $(i, j)$\n",
    "in order to glean relevant information\n",
    "to assess what is going on at $[\\mathbf{H}]_{i, j}$.\n",
    "This means that outside some range $|a|> \\Delta$ or $|b| > \\Delta$,\n",
    "we should set $[\\mathbf{V}]_{a, b} = 0$.\n",
    "Equivalently, we can rewrite $[\\mathbf{H}]_{i, j}$ as\n",
    "\n",
    "$$[\\mathbf{H}]_{i, j} = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$\n",
    "\n",
    "This reduces the number of parameters from $4 \\times 10^6$ to $4 \\Delta^2$, where $\\Delta$ is typically smaller than $10$. As such, we reduced the number of parameters by another four orders of magnitude. Note that this is called a *convolutional layer*.\n",
    "**Convolutional neural networks** (CNNs)\n",
    "are a special family of neural networks that contain convolutional layers.\n",
    "In the deep learning research community,\n",
    "$\\mathbf{V}$ is referred to as a *convolution kernel*,\n",
    "a *filter*, or simply the layer's *weights* that are learnable parameters.\n",
    "\n",
    "While previously, we might have required billions of parameters\n",
    "to represent just a single layer in an image-processing network,\n",
    "we now typically need just a few hundred, without\n",
    "altering the dimensionality of either\n",
    "the inputs or the hidden representations.\n",
    "The price paid for this drastic reduction in parameters\n",
    "is that our features are now translation invariant\n",
    "and that our layer can only incorporate local information,\n",
    "when determining the value of each hidden activation.\n",
    "All learning depends on imposing **inductive bias**.\n",
    "When that bias agrees with reality,\n",
    "we get sample-efficient models\n",
    "that generalize well to unseen data.\n",
    "But of course, if those biases do not agree with reality,\n",
    "e.g., if images turned out not to be translation invariant,\n",
    "our models might struggle even to fit our training data.\n",
    "\n",
    "This dramatic reduction in parameters brings us to our last desideratum,\n",
    "namely that deeper layers should represent larger and more complex aspects\n",
    "of an image. This can be achieved by interleaving nonlinearities and convolutional\n",
    "layers repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Convolutions\n",
    "\n",
    "Let's briefly review why the equation is called a convolution.\n",
    "In mathematics, the *convolution* between two functions,\n",
    "say $f, g: \\mathbb{R}^d \\to \\mathbb{R}$ is defined as\n",
    "\n",
    "$$(f * g)(\\mathbf{x}) = \\int f(\\mathbf{z}) g(\\mathbf{x}-\\mathbf{z}) d\\mathbf{z}.$$\n",
    "\n",
    "That is, we measure the overlap between $f$ and $g$\n",
    "when one function is \"flipped\" and shifted by $\\mathbf{x}$.\n",
    "Whenever we have discrete objects, the integral turns into a sum.\n",
    "For instance, for vectors from\n",
    "the set of square-summable infinite-dimensional vectors\n",
    "with index running over $\\mathbb{Z}$ we obtain the following definition:\n",
    "\n",
    "$$(f * g)(i) = \\sum_a f(a) g(i-a).$$\n",
    "\n",
    "For two-dimensional tensors, we have a corresponding sum\n",
    "with indices $(a, b)$ for $f$ and $(i-a, j-b)$ for $g$, respectively:\n",
    "\n",
    "$$(f * g)(i, j) = \\sum_a\\sum_b f(a, b) g(i-a, j-b).$$\n",
    "\n",
    "This looks similar to the convolutional layer equation, with one major difference.\n",
    "Rather than using $(i+a, j+b)$, we are using the difference instead.\n",
    "Note, though, that this distinction is mostly cosmetic\n",
    "since we can always match the notation between them.\n",
    "Our original definition of the convolutional layer more properly describes a *cross-correlation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Channels\n",
    "So far, we blissfully ignored that images consist\n",
    "of three channels: red, green, and blue.\n",
    "In sum, images are not two-dimensional objects\n",
    "but rather third-order tensors,\n",
    "characterized by a height, width, and channel,\n",
    "e.g., with shape $1024 \\times 1024 \\times 3$ pixels.\n",
    "While the first two of these axes concern spatial relationships,\n",
    "the third can be regarded as assigning\n",
    "a multidimensional representation to each pixel location.\n",
    "We thus index $\\mathsf{X}$ as $[\\mathsf{X}]_{i, j, k}$.\n",
    "The convolutional filter has to adapt accordingly.\n",
    "Instead of $[\\mathbf{V}]_{a,b}$, we now have $[\\mathsf{V}]_{a,b,c}$.\n",
    "\n",
    "Moreover, just as our input consists of a third-order tensor,\n",
    "it turns out to be a good idea to similarly formulate\n",
    "our hidden representations as third-order tensors $\\mathsf{H}$.\n",
    "In other words, rather than just having a single hidden representation\n",
    "corresponding to each spatial location,\n",
    "we want an entire vector of hidden representations\n",
    "corresponding to each spatial location.\n",
    "We could think of the hidden representations as comprising\n",
    "a number of two-dimensional grids stacked on top of each other.\n",
    "As in the inputs, these are sometimes called *channels*.\n",
    "They are also sometimes called *feature maps*,\n",
    "as each provides a spatialized set\n",
    "of learned features for the subsequent layer.\n",
    "Intuitively, you might imagine that at lower layers that are closer to inputs,\n",
    "some channels could become specialized to recognize edges while\n",
    "others could recognize textures.\n",
    "\n",
    "To support multiple channels in both inputs ($\\mathsf{X}$) and hidden representations ($\\mathsf{H}$),\n",
    "we can add a fourth coordinate to $\\mathsf{V}$: $[\\mathsf{V}]_{a, b, c, d}$.\n",
    "Putting everything together we have:\n",
    "\n",
    "$$[\\mathsf{H}]_{i,j,d} = \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} \\sum_c [\\mathsf{V}]_{a, b, c, d} [\\mathsf{X}]_{i+a, j+b, c},$$\n",
    "\n",
    "where $d$ indexes the output channels in the hidden representations $\\mathsf{H}$. The subsequent convolutional layer will go on to take a third-order tensor, $\\mathsf{H}$, as input.\n",
    "This is the definition of a convolutional layer for multiple channels, where $\\mathsf{V}$ is a kernel or filter of the layer.\n",
    "\n",
    "### Multiple Input Channels\n",
    "When the input data contains multiple channels,\n",
    "we need to construct a convolution kernel\n",
    "with the same number of input channels as the input data,\n",
    "so that it can perform cross-correlation with the input data.\n",
    "Assuming that the number of channels for the input data is $c_\\textrm{i}$,\n",
    "the number of input channels of the convolution kernel also needs to be $c_\\textrm{i}$. If our convolution kernel's window shape is $k_\\textrm{h}\\times k_\\textrm{w}$,\n",
    "then, when $c_\\textrm{i}=1$, we can think of our convolution kernel\n",
    "as just a two-dimensional tensor of shape $k_\\textrm{h}\\times k_\\textrm{w}$.\n",
    "\n",
    "However, when $c_\\textrm{i}>1$, we need a kernel\n",
    "that contains a tensor of shape $k_\\textrm{h}\\times k_\\textrm{w}$ for *every* input channel. Concatenating these $c_\\textrm{i}$ tensors together\n",
    "yields a convolution kernel of shape $c_\\textrm{i}\\times k_\\textrm{h}\\times k_\\textrm{w}$.\n",
    "Since the input and convolution kernel each have $c_\\textrm{i}$ channels,\n",
    "we can perform a cross-correlation operation\n",
    "on the two-dimensional tensor of the input\n",
    "and the two-dimensional tensor of the convolution kernel\n",
    "for each channel, adding the $c_\\textrm{i}$ results together\n",
    "(summing over the channels)\n",
    "to yield a two-dimensional tensor.\n",
    "This is the result of a two-dimensional cross-correlation\n",
    "between a multi-channel input and\n",
    "a multi-input-channel convolution kernel.\n",
    "\n",
    "![](https://d2l.ai/_images/conv-multi-in.svg)\n",
    "\n",
    "The above figure provides an example\n",
    "of a two-dimensional cross-correlation with two input channels.\n",
    "The shaded portions are the first output element\n",
    "as well as the input and kernel tensor elements used for the output computation:\n",
    "$(1\\times1+2\\times2+4\\times3+5\\times4)+(0\\times0+1\\times1+3\\times2+4\\times3)=56$.\n",
    "\n",
    "### Multiple Output Channels\n",
    "\n",
    "Regardless of the number of input channels,\n",
    "so far we always ended up with one output channel.\n",
    "However,\n",
    "it turns out to be essential to have multiple channels at each layer.\n",
    "In the most popular neural network architectures,\n",
    "we actually increase the channel dimension\n",
    "as we go deeper in the neural network,\n",
    "typically downsampling to trade off spatial resolution\n",
    "for greater *channel depth*.\n",
    "Intuitively, you could think of each channel\n",
    "as responding to a different set of features.\n",
    "\n",
    "Denote by $c_\\textrm{i}$ and $c_\\textrm{o}$ the number\n",
    "of input and output channels, respectively,\n",
    "and by $k_\\textrm{h}$ and $k_\\textrm{w}$ the height and width of the kernel.\n",
    "To get an output with multiple channels,\n",
    "we can create a kernel tensor\n",
    "of shape $c_\\textrm{i}\\times k_\\textrm{h}\\times k_\\textrm{w}$\n",
    "for *every* output channel.\n",
    "We concatenate them on the output channel dimension,\n",
    "so that the shape of the convolution kernel\n",
    "is $c_\\textrm{o}\\times c_\\textrm{i}\\times k_\\textrm{h}\\times k_\\textrm{w}$.\n",
    "In cross-correlation operations,\n",
    "the result on each output channel is calculated\n",
    "from the convolution kernel corresponding to that output channel\n",
    "and takes input from all channels in the input tensor.\n",
    "\n",
    "## $1\\times 1$ Convolutional Layer\n",
    "At first, a **$1 \\times 1$ convolution**, i.e., $k_\\textrm{h} = k_\\textrm{w} = 1$,\n",
    "does not seem to make much sense.\n",
    "After all, a convolution correlates adjacent pixels.\n",
    "A $1 \\times 1$ convolution obviously does not.\n",
    "Nonetheless, they are popular operations that are sometimes included\n",
    "in the designs of complex deep networks.\n",
    "\n",
    "Because the minimum window is used,\n",
    "the $1\\times 1$ convolution loses the ability\n",
    "of larger convolutional layers\n",
    "to recognize patterns consisting of interactions\n",
    "among adjacent elements in the height and width dimensions.\n",
    "The only computation of the $1\\times 1$ convolution occurs\n",
    "on the channel dimension.\n",
    "\n",
    "![](https://d2l.ai/_images/conv-1x1.svg)\n",
    "\n",
    "The above figure shows the cross-correlation computation\n",
    "using the $1\\times 1$ convolution kernel\n",
    "with 3 input channels and 2 output channels.\n",
    "Note that the inputs and outputs have the same height and width.\n",
    "Each element in the output is derived\n",
    "from a linear combination of elements *at the same position*\n",
    "in the input image.\n",
    "You could think of the $1\\times 1$ convolutional layer\n",
    "as constituting a fully connected layer applied at every single pixel location\n",
    "to transform the $c_\\textrm{i}$ corresponding input values into $c_\\textrm{o}$ output values.\n",
    "Because this is still a convolutional layer,\n",
    "the weights are tied across pixel location.\n",
    "Thus the $1\\times 1$ convolutional layer requires $c_\\textrm{o}\\times c_\\textrm{i}$ weights\n",
    "(plus the bias). Also note that convolutional layers are typically followed\n",
    "by nonlinearities. This ensures that $1 \\times 1$ convolutions cannot simply be\n",
    "folded into other convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cross-Correlation Operation\n",
    "\n",
    "Strictly speaking, convolutional layers\n",
    "are a  misnomer, since the operations they express\n",
    "are more accurately described as cross-correlations.\n",
    "Based on our descriptions of convolutional layers,\n",
    "in such a layer, an input tensor\n",
    "and a kernel tensor are combined\n",
    "to produce an output tensor through a **cross-correlation operation.**\n",
    "\n",
    "Let's ignore channels for now and see how this works\n",
    "with two-dimensional data and hidden representations.\n",
    "\n",
    "![](http://d2l.ai/_images/correlation.svg)\n",
    "\n",
    "The input is a two-dimensional tensor\n",
    "with a height of 3 and width of 3.\n",
    "We mark the shape of the tensor as $3 \\times 3$ or ($3$, $3$).\n",
    "The height and width of the kernel are both 2.\n",
    "The shape of the *kernel window* (or *convolution window*)\n",
    "is given by the height and width of the kernel\n",
    "(here it is $2 \\times 2$).\n",
    "\n",
    "In the two-dimensional cross-correlation operation,\n",
    "we begin with the convolution window positioned\n",
    "at the upper-left corner of the input tensor\n",
    "and slide it across the input tensor,\n",
    "both from left to right and top to bottom.\n",
    "When the convolution window slides to a certain position,\n",
    "the input subtensor contained in that window\n",
    "and the kernel tensor are multiplied elementwise\n",
    "and the resulting tensor is summed up\n",
    "yielding a single scalar value.\n",
    "This result gives the value of the output tensor\n",
    "at the corresponding location.\n",
    "Here, the output tensor has a height of 2 and width of 2\n",
    "and the four elements are derived from\n",
    "the two-dimensional cross-correlation operation:\n",
    "\n",
    "$$\n",
    "0\\times0+1\\times1+3\\times2+4\\times3=19,\\\\\n",
    "1\\times0+2\\times1+4\\times2+5\\times3=25,\\\\\n",
    "3\\times0+4\\times1+6\\times2+7\\times3=37,\\\\\n",
    "4\\times0+5\\times1+7\\times2+8\\times3=43.\n",
    "$$\n",
    "\n",
    "Note that along each axis, the output size\n",
    "is slightly smaller than the input size.\n",
    "Because the kernel has width and height greater than $1$,\n",
    "we can only properly compute the cross-correlation\n",
    "for locations where the kernel fits wholly within the image,\n",
    "the output size is given by the input size $n_\\textrm{h} \\times n_\\textrm{w}$\n",
    "minus the size of the convolution kernel $k_\\textrm{h} \\times k_\\textrm{w}$\n",
    "via\n",
    "\n",
    "$$(n_\\textrm{h}-k_\\textrm{h}+1) \\times (n_\\textrm{w}-k_\\textrm{w}+1).$$\n",
    "\n",
    "This is the case since we need enough space\n",
    "to \"shift\" the convolution kernel across the image.\n",
    "\n",
    "## Convolutional Layers\n",
    "\n",
    "A convolutional layer cross-correlates the input and kernel\n",
    "and adds a scalar bias to produce an output.\n",
    "The two parameters of a convolutional layer\n",
    "are the kernel and the scalar bias.\n",
    "When training models based on convolutional layers,\n",
    "we typically initialize the kernels randomly,\n",
    "just as we would with a fully connected layer.\n",
    "\n",
    "In\n",
    "$h \\times w$ convolution\n",
    "or an $h \\times w$ convolution kernel,\n",
    "the height and width of the convolution kernel are $h$ and $w$, respectively.\n",
    "We also refer to\n",
    "a convolutional layer with an $h \\times w$\n",
    "convolution kernel simply as an $h \\times w$ convolutional layer.\n",
    "\n",
    "## Cross-Correlation and Convolution\n",
    "\n",
    "Consider two-dimensional convolutional layers.\n",
    "What if such layers\n",
    "perform strict convolution operations instead of cross-correlations?\n",
    "In order to obtain the output of the strict *convolution* operation, we only need to flip the two-dimensional kernel tensor both horizontally and vertically, and then perform the *cross-correlation* operation with the input tensor.\n",
    "\n",
    "It is noteworthy that since kernels are learned from data in deep learning,\n",
    "the outputs of convolutional layers remain unaffected\n",
    "no matter such layers perform either the strict convolution operations\n",
    "or the cross-correlation operations.\n",
    "\n",
    "To illustrate this, suppose that a convolutional layer performs *cross-correlation* and learns the kernel, which is here denoted as the matrix $\\mathbf{K}$.\n",
    "Assuming that other conditions remain unchanged,\n",
    "when this layer instead performs strict *convolution*,\n",
    "the learned kernel $\\mathbf{K}'$ will be the same as $\\mathbf{K}$\n",
    "after $\\mathbf{K}'$ is\n",
    "flipped both horizontally and vertically.\n",
    "That is to say,\n",
    "when the convolutional layer\n",
    "performs strict *convolution*\n",
    "for the input\n",
    "and $\\mathbf{K}'$,\n",
    "the same output will be obtained through the cross-correlation of the input and $\\mathbf{K}$.\n",
    "\n",
    "In keeping with standard terminology in deep learning literature,\n",
    "we will continue to refer to the cross-correlation operation\n",
    "as a convolution even though, strictly-speaking, it is slightly different. Furthermore, we use the term element to refer to an entry (or component) of any tensor representing a layer representation or a convolution kernel.\n",
    "\n",
    "## Feature Map and Receptive Field\n",
    "\n",
    "The convolutional layer output\n",
    "is sometimes called a *feature map*,\n",
    "as it can be regarded as\n",
    "the learned representations (features)\n",
    "in the spatial dimensions (e.g., width and height)\n",
    "to the subsequent layer.\n",
    "In CNNs,\n",
    "for any element $x$ of some layer,\n",
    "its *receptive field* refers to\n",
    "all the elements (from all the previous layers)\n",
    "that may affect the calculation of $x$\n",
    "during the forward propagation.\n",
    "Note that the receptive field\n",
    "may be larger than the actual size of the input.\n",
    "\n",
    "![](https://raw.githubusercontent.com/fastai/course22p2/df9323235bc395b5c2f58a3d08b83761947b9b93/nbs/images/att_00068.png)\n",
    "\n",
    "The blue highlighted cells are its precedents—that is, the cells used to calculate its value. These cells are the corresponding 3×3 area of cells from the input layer (on the left), and the cells from the filter (on the right).\n",
    "\n",
    "![](https://raw.githubusercontent.com/fastai/course22p2/df9323235bc395b5c2f58a3d08b83761947b9b93/nbs/images/att_00069.png)\n",
    "\n",
    "In this example, we have just two convolutional layers. We can see that a 7×7 area of cells in the input layer is used to calculate the single green cell in the Conv2 layer. This is the receptive field.\n",
    "\n",
    "The deeper we are in the network, the larger the receptive field for an activation in that layer.\n",
    "\n",
    "![](https://d2l.ai/_images/correlation.svg)\n",
    "\n",
    "Given the $2 \\times 2$ convolution kernel,\n",
    "the receptive field of the shaded output element (of value $19$)\n",
    "is\n",
    "the four elements in the shaded portion of the input.\n",
    "Now let's denote the $2 \\times 2$\n",
    "output as $\\mathbf{Y}$\n",
    "and consider a deeper CNN\n",
    "with an additional $2 \\times 2$ convolutional layer that takes $\\mathbf{Y}$\n",
    "as its input, outputting\n",
    "a single element $z$.\n",
    "In this case,\n",
    "the receptive field of $z$\n",
    "on $\\mathbf{Y}$ includes all the four elements of $\\mathbf{Y}$,\n",
    "while\n",
    "the receptive field\n",
    "on the input includes all the nine input elements.\n",
    "Thus,\n",
    "when any element in a feature map\n",
    "needs a larger receptive field\n",
    "to detect input features over a broader area,\n",
    "we can build a deeper network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:150/format:webp/1*7zVtk9Xb3X6aQScJqxCUYw.png)\n",
    "\n",
    "The filter\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:102/format:webp/1*aCBPCSInHE6vFaECSAT5oA.png)\n",
    "\n",
    "Since the filter fits in the image four times, we have four results\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:106/format:webp/1*I8BERkXPnVjdVCawr7Q23A.png)\n",
    "\n",
    "Here’s how we applied the filter to each section of the image to yield each result\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*s-TmnXo69mAKbnMoMBnpMg.png)\n",
    "\n",
    "The equation view\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*-TnNZ4srYItcGXov-dTmmw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = datasets.MNIST(root='data', download=True, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28>, 7)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = mnist[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APA4Lea6nSC3hkmmc4SONSzMfQAdadc2dzZSmK7tpoJB/BKhU/kahoq1pupXuj6hDf6dcyW13CSY5YzhlyCDj8CRXXWvxe8b20SxtrH2lVOQbqCOU9OmWUn/APVXUfEfxBqCfDzSNJ16S2uNd1JxqEqpbohtIMYjQbQBlsEnv1HpXj9Fdx8OvDNlqNxe+IdeVh4e0VPPucLnznyNkQ/3j1/LjOa57xPr9z4n8R3usXQ2vcSZVB0jQcKo9gABWRRXSxeOdXt/A0nhGAW0WnSzGaZ1j/ey8g7SxOMZA6AHjrXNUV//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAzUlEQVR4AWNgGDaAEeKTkNRnP5a+uIPmLajkPQWg+OerUMknXWfALBYIP1X/mpahg8VjWQaGP68lGR5BJKE6wUoEDc+YMjD8uHVdKGca1AwMKvjvRSEMQaiA2Mv/wVAmE4aabNH3NzEEoQLWP//Z4ZJjaP23mxWXJOfZH1a45Bjq/m3DKef9+4MlLknhu/+W4ZJjPv3vtjIuSbV//3xxyck/+FeMHNYo6lr//TNBEUDi2H5Cl0QKWxsehrtfkBQzMEAjGyJ20fkdiiReDgBpETyQooNMkwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.tv_tensors._image.Image'>\n",
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Image([[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  84, 185, 159, 151,  60,  36,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0, 222, 254, 254, 254, 254, 241, 198, 198,\n",
       "         198, 198, 198, 198, 198, 198, 170,  52,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  67, 114,  72, 114, 163, 227, 254, 225,\n",
       "         254, 254, 254, 250, 229, 254, 254, 140,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  17,  66,  14,\n",
       "          67,  67,  67,  59,  21, 236, 254, 106,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,  83, 253, 209,  18,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,  22, 233, 255,  83,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 129, 254, 238,  44,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,  59, 249, 254,  62,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0, 133, 254, 187,   5,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   9, 205, 248,  58,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0, 126, 254, 182,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          75, 251, 240,  57,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  19,\n",
       "         221, 254, 166,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3, 203,\n",
       "         254, 219,  35,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38, 254,\n",
       "         254,  77,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  31, 224, 254,\n",
       "         115,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 133, 254, 254,\n",
       "          52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  61, 242, 254, 254,\n",
       "          52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 254, 219,\n",
       "          40,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 207,  18,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]],\n",
       "      dtype=torch.uint8, )"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "])\n",
    "\n",
    "image = transform(sample[0])\n",
    "print(type(image))\n",
    "print(image.shape)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.tv_tensors._image.Image'>\n",
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Image([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3294, 0.7255, 0.6235,\n",
       "         0.5922, 0.2353, 0.1412, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8706, 0.9961, 0.9961,\n",
       "         0.9961, 0.9961, 0.9451, 0.7765, 0.7765, 0.7765, 0.7765, 0.7765, 0.7765,\n",
       "         0.7765, 0.7765, 0.6667, 0.2039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2627, 0.4471, 0.2824,\n",
       "         0.4471, 0.6392, 0.8902, 0.9961, 0.8824, 0.9961, 0.9961, 0.9961, 0.9804,\n",
       "         0.8980, 0.9961, 0.9961, 0.5490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0667, 0.2588, 0.0549, 0.2627, 0.2627, 0.2627, 0.2314,\n",
       "         0.0824, 0.9255, 0.9961, 0.4157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.3255, 0.9922, 0.8196, 0.0706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0863,\n",
       "         0.9137, 1.0000, 0.3255, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5059,\n",
       "         0.9961, 0.9333, 0.1725, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2314, 0.9765,\n",
       "         0.9961, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5216, 0.9961,\n",
       "         0.7333, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0353, 0.8039, 0.9725,\n",
       "         0.2275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4941, 0.9961, 0.7137,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2941, 0.9843, 0.9412, 0.2235,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0745, 0.8667, 0.9961, 0.6510, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0118, 0.7961, 0.9961, 0.8588, 0.1373, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.1490, 0.9961, 0.9961, 0.3020, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1216, 0.8784, 0.9961, 0.4510, 0.0039, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.5216, 0.9961, 0.9961, 0.2039, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.2392, 0.9490, 0.9961, 0.9961, 0.2039, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.4745, 0.9961, 0.9961, 0.8588, 0.1569, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.4745, 0.9961, 0.8118, 0.0706, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000]]], )"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "image = transform(sample[0])\n",
    "print(type(image))\n",
    "print(image.shape)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.ToPureTensor(),\n",
    "])\n",
    "\n",
    "image = transform(sample[0])\n",
    "print(type(image))\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3294, 0.7255,\n",
       "           0.6235, 0.5922, 0.2353, 0.1412, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8706, 0.9961,\n",
       "           0.9961, 0.9961, 0.9961, 0.9451, 0.7765, 0.7765, 0.7765, 0.7765,\n",
       "           0.7765, 0.7765, 0.7765, 0.7765, 0.6667, 0.2039, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2627, 0.4471,\n",
       "           0.2824, 0.4471, 0.6392, 0.8902, 0.9961, 0.8824, 0.9961, 0.9961,\n",
       "           0.9961, 0.9804, 0.8980, 0.9961, 0.9961, 0.5490, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0667, 0.2588, 0.0549, 0.2627, 0.2627,\n",
       "           0.2627, 0.2314, 0.0824, 0.9255, 0.9961, 0.4157, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3255, 0.9922, 0.8196, 0.0706, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0863, 0.9137, 1.0000, 0.3255, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.5059, 0.9961, 0.9333, 0.1725, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.2314, 0.9765, 0.9961, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.5216, 0.9961, 0.7333, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0353,\n",
       "           0.8039, 0.9725, 0.2275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4941,\n",
       "           0.9961, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2941, 0.9843,\n",
       "           0.9412, 0.2235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0745, 0.8667, 0.9961,\n",
       "           0.6510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.7961, 0.9961, 0.8588,\n",
       "           0.1373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1490, 0.9961, 0.9961, 0.3020,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.1216, 0.8784, 0.9961, 0.4510, 0.0039,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5216, 0.9961, 0.9961, 0.2039, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2392, 0.9490, 0.9961, 0.9961, 0.2039, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.4745, 0.9961, 0.9961, 0.8588, 0.1569, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.4745, 0.9961, 0.8118, 0.0706, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 7)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_v2 = datasets.MNIST(root='data', download=True, train=False, transform=transform)\n",
    "sample_v2 = mnist_v2[0]\n",
    "sample_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "image = sample_v2[0]\n",
    "print(type(image))\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = image.squeeze()\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.imshow\n",
    "\n",
    "The image data shape:\n",
    "  - (M, N)\n",
    "  - (M, N, 3) (RGB, 0-1 float or 0-255 int)\n",
    "  - (M, N, 4) (RGBA, 0-1 float or 0-255 int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kernel(im, row, col, kernel):\n",
    "  return (im[row-1:row+2, col-1:col+2] * kernel).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_edge = torch.FloatTensor(\n",
    "    [[-1,-1,-1],\n",
    "     [ 0, 0, 0],\n",
    "     [ 1, 1, 1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([26, 26])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = range(1,27)\n",
    "img_top_edge = torch.FloatTensor([[apply_kernel(img, i, j, top_edge) for j in rng] for i in rng])\n",
    "img_top_edge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_top_edge, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), bias=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, bias=False)\n",
    "layer.weight.data = top_edge.reshape(1, 1, 3, 3)\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 26, 26])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_top_edge = layer(image)\n",
    "image_top_edge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image_top_edge.squeeze().detach().numpy(), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(image_top_edge, img_top_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0726, -0.0302, -0.2946],\n",
       "          [ 0.0949, -0.2066,  0.2154],\n",
       "          [ 0.0060, -0.2741,  0.0906]]]], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.1795], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strides and Padding\n",
    "\n",
    "With appropriate padding, we can ensure that the output activation map is the same size as the original image.\n",
    "\n",
    "![](https://raw.githubusercontent.com/fastai/course22p2/df9323235bc395b5c2f58a3d08b83761947b9b93/nbs/images/chapter9_padconv.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (channel, height, width)\n",
    "x = torch.randn(1, 5, 5)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)\n",
    "l(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1)\n",
    "l(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a 5×5 input, 4×4 kernel, and 2 pixels of padding, we end up with a 6×6 activation map.\n",
    "\n",
    "![](https://raw.githubusercontent.com/fastai/course22p2/df9323235bc395b5c2f58a3d08b83761947b9b93/nbs/images/att_00029.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=4, padding=2)\n",
    "l(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could move over two pixels after each kernel application. This is known as a stride-2 convolution.\n",
    "\n",
    "![](https://raw.githubusercontent.com/fastai/course22p2/df9323235bc395b5c2f58a3d08b83761947b9b93/nbs/images/att_00030.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1, stride=2)\n",
    "l(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the input shape is $n_\\textrm{h}\\times n_\\textrm{w}$\n",
    "and the convolution kernel shape is $k_\\textrm{h}\\times k_\\textrm{w}$,\n",
    "the output shape will be $(n_\\textrm{h}-k_\\textrm{h}+1) \\times (n_\\textrm{w}-k_\\textrm{w}+1)$:\n",
    "we can only shift the convolution kernel so far until it runs out\n",
    "of pixels to apply the convolution to.\n",
    "\n",
    "In general, if we add a total of $p_\\textrm{h}$ rows of padding\n",
    "(roughly half on top and half on bottom)\n",
    "and a total of $p_\\textrm{w}$ columns of padding\n",
    "(roughly half on the left and half on the right),\n",
    "the output shape will be\n",
    "\n",
    "$$(n_\\textrm{h}-k_\\textrm{h}+p_\\textrm{h}+1)\\times(n_\\textrm{w}-k_\\textrm{w}+p_\\textrm{w}+1).$$\n",
    "\n",
    "This means that the height and width of the output\n",
    "will increase by $p_\\textrm{h}$ and $p_\\textrm{w}$, respectively.\n",
    "\n",
    "In many cases, we will want to set $p_\\textrm{h}=k_\\textrm{h}-1$ and $p_\\textrm{w}=k_\\textrm{w}-1$\n",
    "to give the input and output the same height and width.\n",
    "\n",
    "Assuming that $k_\\textrm{h}$ is odd here,\n",
    "we will pad $p_\\textrm{h}/2$ rows on both sides of the height.\n",
    "If $k_\\textrm{h}$ is even, one possibility is to\n",
    "pad $\\lceil p_\\textrm{h}/2\\rceil$ rows on the top of the input\n",
    "and $\\lfloor p_\\textrm{h}/2\\rfloor$ rows on the bottom.\n",
    "We will pad both sides of the width in the same way.\n",
    "\n",
    "CNNs commonly use convolution kernels\n",
    "with odd height and width values, such as 1, 3, 5, or 7.\n",
    "Choosing odd kernel sizes has the benefit\n",
    "that we can preserve the dimensionality\n",
    "while padding with the same number of rows on top and bottom,\n",
    "and the same number of columns on left and right.\n",
    "\n",
    "Moreover, this practice of using odd kernels\n",
    "and padding to precisely preserve dimensionality\n",
    "offers a clerical benefit.\n",
    "For any two-dimensional tensor `X`,\n",
    "when the kernel's size is odd\n",
    "and the number of padding rows and columns\n",
    "on all sides are the same,\n",
    "thereby producing an output with the same height and width as the input,\n",
    "we know that the output `Y[i, j]` is calculated\n",
    "by cross-correlation of the input and convolution kernel\n",
    "with the window centered on `X[i, j]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "In many cases our ultimate task asks some global question about the image, e.g., does it contain a cat? Consequently, the units of our final layer should be sensitive to the entire input. By gradually aggregating information, yielding coarser and coarser maps, we accomplish this goal of ultimately learning a global representation, while keeping all of the advantages of convolutional layers at the intermediate layers of processing. The deeper we go in the network, the larger the receptive field (relative to the input) to which each hidden node is sensitive. Reducing spatial resolution accelerates this process, since the convolution kernels cover a larger effective area.\n",
    "\n",
    "### Maximum Pooling and Average Pooling\n",
    "\n",
    "Like convolutional layers, pooling operators consist of a fixed-shape window that is slid over all regions in the input according to its stride, computing a single output for each location traversed by the fixed-shape window (sometimes known as the pooling window). However, unlike the cross-correlation computation of the inputs and kernels in the convolutional layer, the pooling layer contains no parameters (there is no kernel). Instead, pooling operators are deterministic, typically calculating either the maximum or the average value of the elements in the pooling window. These operations are called maximum pooling (max-pooling for short) and average pooling, respectively.\n",
    "\n",
    "We can think of the pooling window as starting from the upper-left of the input tensor and sliding across it from left to right and top to bottom. At each location that the pooling window hits, it computes the maximum or average value of the input subtensor in the window, depending on whether max or average pooling is employed.\n",
    "\n",
    "![](https://d2l.ai/_images/pooling.svg)\n",
    "\n",
    "- Max-pooling with a pooling window shape of $2 \\times 2$. The shaded portions are the first output element as well as the input tensor elements used for the output computation: $\\text{max}(0, 1, 3, 4) = 4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 2.],\n",
       "         [3., 4., 5.],\n",
       "         [6., 7., 8.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(9, dtype=torch.float32).reshape((1, 3, 3))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4., 5.],\n",
       "         [7., 8.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(2, stride=1)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding and Stride\n",
    "\n",
    "As with convolutional layers, pooling layers\n",
    "change the output shape.\n",
    "And as before, we can adjust the operation to achieve a desired output shape\n",
    "by padding the input and adjusting the stride.\n",
    "We can demonstrate the use of padding and strides\n",
    "in pooling layers via the built-in two-dimensional max-pooling layer from the deep learning framework.\n",
    "We first construct an input tensor `X` whose shape has four dimensions,\n",
    "where the number of examples (batch size) and number of channels are both 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pooling aggregates information from an area, **deep learning frameworks default to matching pooling window sizes and stride.** For instance, if we use a pooling window of shape `(3, 3)`\n",
    "we get a stride shape of `(3, 3)` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[10.]]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needless to say, **the stride and padding can be manually specified** to override framework defaults if required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can specify an arbitrary rectangular pooling window with arbitrary height and width respectively, as the example below shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Channels\n",
    "\n",
    "When processing multi-channel input data,\n",
    "**the pooling layer pools each input channel separately**,\n",
    "rather than summing the inputs up over channels\n",
    "as in a convolutional layer.\n",
    "This means that the number of output channels for the pooling layer\n",
    "is the same as the number of input channels.\n",
    "Below, we will concatenate tensors `X` and `X + 1`\n",
    "on the channel dimension to construct an input with two channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]],\n",
       "\n",
       "         [[ 1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12.],\n",
       "          [13., 14., 15., 16.]]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.cat((X, X + 1), 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the number of output channels is still two after pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]],\n",
       "\n",
       "         [[ 6.,  8.],\n",
       "          [14., 16.]]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
