{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons\n",
    "\n",
    "- https://d2l.ai/chapter_multilayer-perceptrons/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest deep networks are called multilayer perceptrons, and they consist of multiple layers of neurons each fully connected to those in the layer below (from which they receive input) and those above (which they, in turn, influence).\n",
    "\n",
    "The **linear** model is:\n",
    "\n",
    "$$\n",
    "y = Wx + b\n",
    "$$\n",
    "\n",
    "- $y \\in \\mathbb{R}^{\\text{out_features}}$\n",
    "- $x \\in \\mathbb{R}^{\\text{in_features}}$\n",
    "- $W \\in \\mathbb{R}^{\\text{in_features} \\times \\text{out_features}}$\n",
    "- $b \\in \\mathbb{R}^{\\text{out_features}}$\n",
    "\n",
    "Strictly speaking, this is an *affine transformation* of input features, which is characterized by a linear transformation of features via a weighted sum, combined with a *translation* via the added bias.\n",
    "\n",
    "[If we consider a batch size](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html),\n",
    "\n",
    "$$\n",
    "y = xW + b\n",
    "$$\n",
    "\n",
    "- $y \\in \\mathbb{R}^{\\text{batch_size} \\times \\text{out_features}}$\n",
    "- $x \\in \\mathbb{R}^{\\text{batch_size} \\times \\text{in_features}}$\n",
    "- $W \\in \\mathbb{R}^{\\text{in_features} \\times \\text{out_features}}$\n",
    "- $b \\in \\mathbb{R}^{\\text{out_features}}$\n",
    "\n",
    "In PyTorch, $A^T=W$\n",
    "\n",
    "$$\n",
    "y = xA^T + b\n",
    "$$\n",
    "\n",
    "- $y \\in \\mathbb{R}^{\\text{batch_size} \\times \\text{out_features}}$\n",
    "- $x \\in \\mathbb{R}^{\\text{batch_size} \\times \\text{in_features}}$\n",
    "- $A \\in \\mathbb{R}^{\\text{out_features} \\times \\text{in_features}}$\n",
    "- $b \\in \\mathbb{R}^{\\text{out_features}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Linear neural networks\n",
    "\n",
    "![](https://d2l.ai/_images/softmaxreg.svg)\n",
    "- A single-layer neural network\n",
    "\n",
    "$$\n",
    "o = xW + b\n",
    "$$\n",
    "\n",
    "This model maps inputs directly to outputs via a single affine transformation.\n",
    "\n",
    "If our labels truly were related to the input data by a simple affine transformation, then this approach would be sufficient. However, linearity (in affine transformations) is a strong assumption.\n",
    "\n",
    "For example, linearity implies the weaker assumption of monotonicity, i.e., that any increase in our feature must either always cause an increase in our model's output (if the corresponding weight is positive), or always cause a decrease in our model's output (if the corresponding weight is negative).\n",
    "\n",
    "But what about classifying images of cats and dogs? Should increasing the intensity of the pixel at location (13, 17) always increase (or always decrease) the likelihood that the image depicts a dog? Reliance on a linear model corresponds to the implicit assumption that the only requirement for differentiating cats and dogs is to assess the brightness of individual pixels. This approach is doomed to fail in a world where inverting an image preserves the category.\n",
    "\n",
    "## Incorporating Hidden Layers\n",
    "\n",
    "We can overcome the limitations of linear models by incorporating one or more hidden layers. The easiest way to do this is to stack many fully connected layers on top of one another. Each layer feeds into the layer above it, until we generate outputs.\n",
    "\n",
    "We can think of the first $L-1$ layers as our representation and the final layer as our linear predictor. This architecture is commonly called a multilayer perceptron, often abbreviated as MLP.\n",
    "\n",
    "![](https://d2l.ai/_images/mlp.svg)\n",
    "- A two-layer MLP with a hidden layer of five hidden units.\n",
    "\n",
    "This MLP has four inputs, three outputs, and its hidden layer contains five hidden units. Since the input layer does not involve any calculations, producing outputs with this network requires implementing the computations for both the hidden and output layers; thus, the number of layers in this MLP is two. Note that both layers are fully connected. Every input influences every neuron in the hidden layer, and each of these in turn influences every neuron in the output layer.\n",
    "\n",
    "## From Linear to Nonlinear\n",
    "\n",
    "We denote by the matrix $x \\in \\mathbb{R}^{n \\times d}$ a minibatch of $n$ examples where each example has $d$ inputs (features). For a one-hidden-layer MLP whose hidden layer has $r$ hidden units, we denote by $h \\in \\mathbb{R}^{n \\times r}$ the outputs of the hidden layer, which are **hidden representations**. Since the hidden and output layers are both fully connected, we have hidden-layer weights $W^{(1)} \\in \\mathbb{R}^{d \\times r}$ and biases $b^{(1)}\\in \\mathbb{R}^{1 \\times r}$ and output-layer weights $W^{(2)} \\in \\mathbb{R}^{r \\times q}$ and biases $b^{(2)}\\in \\mathbb{R}^{1 \\times q}$. This allows us to calculate the outputs $o \\in \\mathbb{R}^{n \\times q}$ of the one-hidden-layer MLP as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h & = xW^{(1)} + b^{(1)} \\\\\n",
    "o & = hW^{(2)} + b^{(2)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The hidden units above are given by an affine function of the inputs, and the outputs are just an affine function of the hidden units. An affine function of an affine function is itself an affine function.\n",
    "\n",
    "To see this formally we can just collapse out the hidden layer in the above definition, yielding an equivalent single-layer model with parameters $W=W^{(1)}W^{(2)}$ and $b=b^{(1)}W^{(2)}+b^{(2)}$\n",
    "\n",
    "$$\n",
    "o = (xW^{(1)} + b^{(1)})W^{(2)} + b^{(2)} = xW^{(1)}W^{(2)} + b^{(1)}W^{(2)} + b^{(2)} = xW + b\n",
    "$$\n",
    "\n",
    "In order to realize the potential of multilayer architectures, we need one more key ingredient: a nonlinear **activation function** $\\sigma$ to be applied to each hidden unit following the affine transformation. For instance, a popular choice is the ReLU (rectified linear unit) activation function $\\sigma(x)=\\text{max}(0, x)$ operating on its arguments elementwise. The outputs of activation functions $\\sigma(\\cdot)$ are called **activations**. In general, with activation functions in place, it is no longer possible to collapse our MLP into a linear model (2-layer MLP):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h & = \\sigma(xW^{(1)} + b^{(1)}) \\\\\n",
    "o & = hW^{(2)} + b^{(2)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since each row in $x$ corresponds to an example in the minibatch, with some abuse of notation, we define the nonlinearity $\\sigma$ to apply to its inputs in a rowwise fashion, i.e., one example at a time. Quite frequently the activation functions we use apply not merely rowwise but elementwise. That means that after computing the linear portion of the layer, we can calculate each activation without looking at the values taken by the other hidden units.\n",
    "\n",
    "To build more general MLPs, we can continue stacking such hidden layers (L-layer MLP):\n",
    "$$\n",
    "\\begin{align}\n",
    "h^{(1)} & = \\sigma_1(xW^{(1)} + b^{(1)}) \\\\\n",
    "h^{(2)} & = \\sigma_2(h^{(1)}W^{(2)} + b^{(2)}) \\\\\n",
    "& \\vdots \\\\\n",
    "h^{(l)} & = \\sigma_l(h^{(l-1)}W^{(l)} + b^{(l)}) \\\\\n",
    "& \\vdots \\\\\n",
    "o & = h^{L-1}W^{(L)} + b^{(L)} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "d = 3\n",
    "r = 4\n",
    "q = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "# Input (n, d)\n",
    "x = torch.randn(n, d)\n",
    "print(x.shape)\n",
    "\n",
    "# Single-layer neural network\n",
    "layer = nn.Linear(d, q)\n",
    "\n",
    "# Output (n, q)\n",
    "o = layer(x)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (q, d)\n",
    "W = layer.weight\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (q)\n",
    "b = layer.bias\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8830, -0.2893],\n",
       "        [-0.1364, -0.3266],\n",
       "        [-0.8438, -1.0263],\n",
       "        [-0.0796,  0.4292],\n",
       "        [-0.2859,  0.1128]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8830, -0.2893],\n",
       "        [-0.1364, -0.3266],\n",
       "        [-0.8438, -1.0263],\n",
       "        [-0.0796,  0.4292],\n",
       "        [-0.2859,  0.1128]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ W.T + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-layer MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d, r, q):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(d, r),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(r, q)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(d, r, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "o = model(x)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=4, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=4, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3, out_features=4, bias=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = model.layers[0]\n",
    "layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLU()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig = model.layers[1]\n",
    "sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4, out_features=2, bias=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer2 = model.layers[2]\n",
    "layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (r, d)\n",
    "W1 = layer1.weight\n",
    "W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (r)\n",
    "b1 = layer1.bias\n",
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (q, r)\n",
    "W2 = layer2.weight\n",
    "W2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (q)\n",
    "b2 = layer2.bias\n",
    "b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0061,  0.8154],\n",
       "        [-0.5148,  0.4338],\n",
       "        [-0.4470,  0.4068],\n",
       "        [-0.2470,  0.5521],\n",
       "        [-0.2604,  0.5423]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0061,  0.8154],\n",
       "        [-0.5148,  0.4338],\n",
       "        [-0.4470,  0.4068],\n",
       "        [-0.2470,  0.5521],\n",
       "        [-0.2604,  0.5423]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = sig(x @ W1.T + b1)\n",
    "h @ W2.T + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
