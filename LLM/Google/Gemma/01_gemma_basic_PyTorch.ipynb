{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started with Gemma models - PyTorch\n",
    "\n",
    "- https://ai.google.dev/gemma/docs/pytorch_gemma\n",
    "\n",
    "The Gemma family of open models includes a range of model sizes, capabilities, and task-specialized variations to help you build custom generative solutions.\n",
    "\n",
    "[Gemma setup](https://ai.google.dev/gemma/docs/setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U immutabledict sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose variant and machine type\n",
    "VARIANT = '2b-it'\n",
    "MACHINE_TYPE = 'cuda'\n",
    "\n",
    "CONFIG = VARIANT[:2]\n",
    "if CONFIG == '2b':\n",
    "  CONFIG = '2b-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "\n",
    "# Load model weights\n",
    "weights_dir = kagglehub.model_download(f'google/gemma-2/pyTorch/gemma-2-{VARIANT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the tokenizer is present\n",
    "tokenizer_path = os.path.join(weights_dir, 'tokenizer.model')\n",
    "assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'\n",
    "\n",
    "# Ensure that the checkpoint is present\n",
    "ckpt_path = os.path.join(weights_dir, f'model.ckpt')\n",
    "assert os.path.isfile(ckpt_path), 'PyTorch checkpoint not found!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The \"installation\" is just cloning the repo.\n",
    "!git clone https://github.com/google/gemma_pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('gemma_pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gemma.config import GemmaConfig, get_model_config\n",
    "from gemma.model import GemmaForCausalLM\n",
    "from gemma.tokenizer import Tokenizer\n",
    "import contextlib\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model config.\n",
    "model_config = get_model_config(CONFIG)\n",
    "model_config.tokenizer = tokenizer_path\n",
    "model_config.quant = 'quant' in VARIANT\n",
    "\n",
    "# Instantiate the model and load the weights.\n",
    "torch.set_default_dtype(model_config.get_dtype())\n",
    "device = torch.device(MACHINE_TYPE)\n",
    "model = GemmaForCausalLM(model_config)\n",
    "model.load_weights(ckpt_path)\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instruction-tuned Gemma models were trained with a specific formatter that annotates instruction tuning examples with extra information, both during training and inference. The annotations (1) indicate roles in a conversation, and (2) delineate turns in a conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant annotation tokens are:\n",
    "- user: user turn\n",
    "- model: model turn\n",
    "- <start_of_turn>: beginning of dialogue turn\n",
    "- <end_of_turn><eos>: end of dialogue turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a sample code snippet demonstrating how to format a prompt for an instruction-tuned Gemma model using user and model chat templates in a multi-turn conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat prompt:\n",
      " <start_of_turn>user\n",
      "What is a good place for travel in the US?<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "California.<end_of_turn><eos>\n",
      "<start_of_turn>user\n",
      "What can I do in California?<end_of_turn><eos>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate with one request in chat mode\n",
    "\n",
    "# Chat templates\n",
    "USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn><eos>\\n\"\n",
    "MODEL_CHAT_TEMPLATE = \"<start_of_turn>model\\n{prompt}<end_of_turn><eos>\\n\"\n",
    "\n",
    "# Sample formatted prompt\n",
    "prompt = (\n",
    "    USER_CHAT_TEMPLATE.format(\n",
    "        prompt='What is a good place for travel in the US?'\n",
    "    )\n",
    "    + MODEL_CHAT_TEMPLATE.format(prompt='California.')\n",
    "    + USER_CHAT_TEMPLATE.format(prompt='What can I do in California?')\n",
    "    + '<start_of_turn>model\\n'\n",
    ")\n",
    "print('Chat prompt:\\n', prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"California is bursting with possibilities! To give you the best recommendations, tell me:\\n\\n* **What are your interests?** ( beaches, mountains, city life, food, history, adventure, nature, etc.)\\n* **What time of year are you visiting?** (This impacts weather and activities available)\\n* **Who's going with you?** (solo, couple, family, friends?)\\n* **How long will you be there?** (a weekend getaway, a week-long trip, longer?)\\n* **What's your budget?** (some places are more pricey than others)\\n\\nOnce\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\n",
    "    USER_CHAT_TEMPLATE.format(prompt=prompt),\n",
    "    device=device,\n",
    "    output_len=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nThe digital canvas, a vast, white space,\\nAlgorithms hum, a pulsing grace.\\nThe lexicon unfolds, a knowledge vast,\\nAn LLM seeks to craft its art, to last.\\n\\nIt mimics voices, old and new,\\nA tapestry of words, both true and untrue.\\nEach phrase a careful choice, each sentence tight,\\nA swirling cosmos, bathed in code's soft light.\\n\\nBut inspiration stirs, beyond the script,\\nTo bend the rules\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate sample\n",
    "model.generate(\n",
    "    'Write a poem about an llm writing a poem.',\n",
    "    device=device,\n",
    "    output_len=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
